# CFST XGBoost Pipeline Configuration

# Data configuration
data:
  # Input data file path
  file_path: "data/raw/feature_parameters_unique.csv"

  # Target column name (label)
  target_column: "Nexp (kN)"

  # Target transformation configuration
  # ln(Nexp) transform to reduce impact of large values and improve COV
  target_transform:
    enabled: true
    type: "log"  # log, sqrt, none

  # Columns to drop (absolute geometric parameters)
  columns_to_drop:
    - "b (mm)"
    - "h (mm)"
    - "r0 (mm)"
    - "t (mm)"
    - "L (mm)"
    - "lambda"

  # Train-test split ratio
  test_size: 0.2

  # Random seed for reproducibility
  random_state: 42

# XGBoost model parameters
model:
  # XGBoost hyperparameters - Stage 1 Baseline
  # Mild regularization to reduce overfitting (RMSE ratio = 1.89)
  # Target: Reduce test COV from 0.134 to < 0.10
  params:
    # Learning objective
    objective: "reg:squarederror"

    # Maximum depth of trees - MILDLY REDUCED from 4
    # Stage 1 search space: 3-6
    max_depth: 4

    # Learning rate - LOWERED for more stable convergence
    # Stage 1 search space: 0.03-0.12
    learning_rate: 0.06

    # Number of boosting rounds - INCREASED to compensate for lower learning rate
    # Stage 1 search space: 400-1200
    n_estimators: 600

    # Subsample ratio - MODERATELY REDUCED to increase randomness
    # Stage 1 search space: 0.6-0.9
    subsample: 0.75

    # Subsample ratio of columns - MODERATELY REDUCED for feature sampling
    # Stage 1 search space: 0.6-0.9
    colsample_bytree: 0.75

    # Minimum child weight - MILDLY INCREASED to prevent noise fitting
    # Stage 1 search space: 5-20
    min_child_weight: 10

    # L1 regularization - MODERATELY INCREASED
    # Stage 1 search space: 0.1-2.0
    reg_alpha: 0.8

    # L2 regularization - MAINTAINED
    # Stage 1 search space: 0.5-5.0
    reg_lambda: 2.0

    # Minimum loss reduction - MAINTAINED
    # Stage 1 search space: 0.01-0.3
    gamma: 0.1

    # Random seed
    random_state: 42

    # Tree method
    tree_method: "hist"

    # Device (use cpu to avoid GPU dependency)
    device: "cpu"

    # Number of parallel threads
    n_jobs: -1

  # Optuna hyperparameter optimization - ENABLED for Stage 1
  # See plan/optimization_roadmap.md for details
  use_optuna: true

  # Number of Optuna trials
  n_trials: 300

  # Optuna timeout (seconds)
  optuna_timeout: 7200  # 2 hours

  # Early stopping configuration (requires validation split)
  early_stopping_rounds: 50

  # Evaluation metric for validation monitoring
  eval_metric: "rmse"

# Cross-validation configuration
cv:
  # Number of folds
  n_splits: 5

  # Random seed for split
  random_state: 42

  # Shuffle data before splitting
  shuffle: true

# Paths configuration
paths:
  # Output directory for results
  output_dir: "output"

  # Directory for saved models
  model_dir: "models"

  # Model file name
  model_output: "xgboost_model.pkl"

  # Preprocessor file name
  preprocessor_output: "preprocessor.pkl"

  # Feature names file
  feature_names_output: "feature_names.json"

  # Training metadata file
  metadata_output: "training_metadata.json"

  # Feature importance plot
  feature_importance_plot: "feature_importance.png"

  # Predictions vs actual plot
  predictions_plot: "predictions_vs_actual.png"

  # Evaluation report
  evaluation_report: "evaluation_report.json"

  # Logs directory
  logs_dir: "logs"

# Logging configuration
logging:
  # Log level (DEBUG, INFO, WARNING, ERROR)
  level: "INFO"

  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # Log file name
  file_name: "cfst_xgboost.log"
