# CFSTæŸ±æé™æ‰¿è½½åŠ›é¢„æµ‹ - XGBoost MLç®¡é“

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ ç®¡é“ï¼Œç”¨äºé¢„æµ‹æ··å‡åœŸå¡«å……é’¢ç®¡ï¼ˆCFSTï¼‰æŸ±çš„æé™æ‰¿è½½åŠ›ã€‚é‡‡ç”¨XGBoostç®—æ³•ï¼Œæä¾›ä»æ•°æ®åŠ è½½ã€é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°åˆ°é¢„æµ‹çš„å…¨æµç¨‹è§£å†³æ–¹æ¡ˆã€‚

### æ ¸å¿ƒç‰¹æ€§

- **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¸…æ™°çš„æ¨¡å—åˆ†ç¦»ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
- **ç‰¹å¾å·¥ç¨‹æ”¯æŒ**ï¼šå¤„ç†æ— é‡çº²å‚æ•°ï¼Œå‰”é™¤å‡ ä½•å‚æ•°å®ç°å¤šæˆªé¢ç»Ÿä¸€é¢„æµ‹
- **è‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–**ï¼šé›†æˆOptunaæ¡†æ¶è¿›è¡Œè¶…å‚æ•°æœç´¢
- **ç‰¹å¾é€‰æ‹©**ï¼šè¿­ä»£å‰”é™¤æ³•è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜ç‰¹å¾å­é›†
- **å¤šç»´åº¦è¯„ä¼°**ï¼šé›†æˆRÂ²ã€RMSEã€MAEã€MAPEã€COVç­‰å·¥ç¨‹æŒ‡æ ‡
- **å¯è§†åŒ–åˆ†æ**ï¼šæä¾›é¢„æµ‹æ•£ç‚¹å›¾ã€ç‰¹å¾é‡è¦æ€§å›¾å’Œå­¦ä¹ æ›²çº¿
- **COVç¨³å®šæ€§æŒ‡æ ‡**ï¼šç‰¹åˆ«æ·»åŠ å˜å¼‚ç³»æ•°è¯„ä¼°æ¨¡å‹é¢„æµ‹ç¨³å®šæ€§

---

## ç¯å¢ƒè¦æ±‚

### ç³»ç»Ÿè¦æ±‚
- Python 3.8+
- 8GB+ RAMï¼ˆæ¨è16GBï¼‰
- 5GB+ å¯ç”¨ç£ç›˜ç©ºé—´

### Pythonä¾èµ–
- pandas &gt;= 1.5.0
- numpy &gt;= 1.23.0
- xgboost &gt;= 1.7.0
- scikit-learn &gt;= 1.2.0
- matplotlib &gt;= 3.6.0
- seaborn &gt;= 0.12.0
- optuna &gt;= 3.0.0
- joblib &gt;= 1.2.0

---

## é¡¹ç›®ç»“æ„

```
xgboost/
â”œâ”€â”€ config/                           # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ config.yaml                   # ä¸»é…ç½®ï¼ˆå‰”é™¤å‡ ä½•å‚æ•°ï¼‰
â”‚   â””â”€â”€ config_all_features.yaml      # å…¨é‡ç‰¹å¾é…ç½®
â”œâ”€â”€ data/                             # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/                          # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/                    # å¤„ç†åæ•°æ®
â”‚   â””â”€â”€ models/                       # è®­ç»ƒå¥½çš„æ¨¡å‹
â”œâ”€â”€ output/                           # è¾“å‡ºç»“æœç›®å½•
â”‚   â”œâ”€â”€ xgboost_model/                # æ¨¡å‹è¾“å‡º
â”‚   â”œâ”€â”€ feature_selection/            # ç‰¹å¾é€‰æ‹©ç»“æœï¼ˆ18å‚æ•°ï¼‰
â”‚   â”œâ”€â”€ feature_selection_all/        # ç‰¹å¾é€‰æ‹©ç»“æœï¼ˆå«å‡ ä½•å‚æ•°ï¼‰
â”‚   â”œâ”€â”€ feature_selection_with_cov/   # å¸¦COVçš„ç‰¹å¾é€‰æ‹©
â”‚   â””â”€â”€ feature_selection_comparison.csv
â”œâ”€â”€ src/                              # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ data_loader.py                # æ•°æ®åŠ è½½æ¨¡å—
â”‚   â”œâ”€â”€ preprocessor.py               # æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ model_trainer.py              # æ¨¡å‹è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ evaluator.py                  # æ¨¡å‹è¯„ä¼°å™¨
â”‚   â”œâ”€â”€ visualizer.py                 # å¯è§†åŒ–æ¨¡å—
â”‚   â”œâ”€â”€ predictor.py                  # é¢„æµ‹æ¨¡å—
â”‚   â””â”€â”€ utils/                        # å·¥å…·æ¨¡å—
â”œâ”€â”€ train.py                          # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ predict.py                        # é¢„æµ‹è„šæœ¬
â”œâ”€â”€ feature_selection_pipeline.py     # ç‰¹å¾é€‰æ‹©ç®¡é“
â”œâ”€â”€ requirements.txt                  # Pythonä¾èµ–
â””â”€â”€ README.md                         # æœ¬æ–‡æ¡£
```

---

## æ•´ä½“æ¶æ„æµç¨‹å›¾

```mermaid
graph TD
    A[åŸå§‹æ•°æ®] --> B[æ•°æ®åŠ è½½å™¨<br/>DataLoader]
    B --> C[å‚æ•°æ£€æŸ¥]
    C --> D[æ•°æ®åˆ†å‰²<br/>train/test]
    D --> E[é¢„å¤„ç†å™¨<br/>Preprocessor]
    E --> F[åˆ—å‰”é™¤<br/>å‰”é™¤å‡ ä½•å‚æ•°]
    F --> G[ç¼ºå¤±å€¼å¡«å……<br/>ä¸­ä½æ•°æ’è¡¥]
    G --> H[æ¨¡å‹è®­ç»ƒå™¨<br/>ModelTrainer]
    H --> I[è¶…å‚æ•°ä¼˜åŒ–<br/>Optuna]
    I --> J[XGBoostè®­ç»ƒ]
    J --> K[æ¨¡å‹ä¿å­˜<br/>model.pkl]
    K --> L[è¯„ä¼°å™¨<br/>Evaluator]
    L --> M[å¤šæŒ‡æ ‡è®¡ç®—<br/>RÂ²/RMSE/COV]
    M --> N[å¯è§†åŒ–å™¨<br/>Visualizer]
    N --> O[ç”ŸæˆæŠ¥å‘Š<br/>å›¾ç‰‡/JSON]
    O --> P[é¢„æµ‹å™¨<br/>Predictor]
    P --> Q[æ‰¹é‡é¢„æµ‹<br/>CSVè¾“å‡º]

    subgraph ç‰¹å¾é€‰æ‹©ç®¡é“
        K --> R[è¿­ä»£è®­ç»ƒ]
        R --> S[å‰”é™¤é‡è¦æ€§æœ€ä½ç‰¹å¾]
        S --> T[æ€§èƒ½æ›²çº¿]
        T --> U[æœ€ä¼˜å­é›†é€‰æ‹©]
    end
```

---

## ç³»ç»Ÿæ—¶åºå›¾

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Train as train.py
    participant Loader as DataLoader
    participant Prep as Preprocessor
    participant Trainer as ModelTrainer
    participant Evaluator as Evaluator
    participant Visualizer as Visualizer
    participant Predictor as Predictor

    Note over User,Predictor: è®­ç»ƒæµç¨‹
    User->>Train: æ‰§è¡Œè®­ç»ƒå‘½ä»¤
    Train->>Loader: load_data(è·¯å¾„, ç›®æ ‡åˆ—)
    Loader-->>Train: ç‰¹å¾æ•°æ®, ç›®æ ‡å€¼
    Train->>Prep: fit_transform(æ•°æ®, å‰”é™¤åˆ—)
    Prep-->>Train: å¤„ç†åæ•°æ®, ç‰¹å¾å
    Train->>Trainer: train(è®­ç»ƒæ•°æ®, æµ‹è¯•æ•°æ®, è¶…å‚æ•°)
    Trainer->>Trainer: cross-validation
    Trainer->>Trainer: Optunaä¼˜åŒ–
    Trainer-->>Train: è®­ç»ƒå¥½çš„æ¨¡å‹
    Train->>Evaluator: evaluate(y_true, y_pred)
    Evaluator-->>Train: RÂ², RMSE, COVç­‰
    Train->>Visualizer: plot_predictions(çœŸå®å€¼, é¢„æµ‹å€¼)
    Visualizer-->>Train: æ•£ç‚¹å›¾
    Train->>Visualizer: plot_feature_importance(ç‰¹å¾é‡è¦æ€§)
    Visualizer-->>Train: ç‰¹å¾é‡è¦æ€§å›¾
    Train->>Train: save_model(æ¨¡å‹, é¢„å¤„ç†å™¨, é…ç½®)
    Train-->>User: è®­ç»ƒå®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆ

    Note over User,Predictor: é¢„æµ‹æµç¨‹
    User->>Predictor: predict_single(è¾“å…¥å­—å…¸)
    Predictor->>Predictor: åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å™¨
    Predictor->>Evaluator: calculate_cov(y_true, y_pred)
    Evaluator-->>Predictor: COVå€¼
    Predictor-->>User: é¢„æµ‹ç»“æœ + ç½®ä¿¡åº¦

    Note over User,Predictor: ç‰¹å¾é€‰æ‹©æµç¨‹
    User->>Train: æ‰§è¡Œç‰¹å¾é€‰æ‹©å‘½ä»¤
    loop è¿­ä»£è®­ç»ƒ
        Train->>Trainer: train_and_evaluate(å½“å‰ç‰¹å¾é›†)
        Trainer-->>Train: æ¨¡å‹ + æ€§èƒ½æŒ‡æ ‡
        Train->>Train: ç§»é™¤é‡è¦æ€§æœ€ä½ç‰¹å¾
    end
    Train->>Visualizer: plot_feature_selection_curve(ç»“æœ)
    Visualizer-->>Train: ç‰¹å¾é€‰æ‹©æ›²çº¿
    Train-->>User: æœ€ä¼˜ç‰¹å¾é›†æ¨è
```

---

## é…ç½®æ–‡ä»¶è¯¦è§£

### ä¸»é…ç½®æ–‡ä»¶ï¼ˆconfig.yamlï¼‰

```yaml
# æ•°æ®é…ç½®
data:
  train_path: "feature_parameters.csv"         # è®­ç»ƒæ•°æ®è·¯å¾„
  target_column: "Nexp (kN)"                   # ç›®æ ‡åˆ—ï¼ˆæ‰¿è½½åŠ›ï¼‰
  test_size: 0.2                               # æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆ20%ï¼‰
  random_state: 42                             # éšæœºç§å­

  # å‰”é™¤çš„åˆ—ï¼ˆå‡ ä½•å‚æ•°ï¼‰
  columns_to_drop:                             #
    - "b (mm)"                                # æˆªé¢å®½åº¦
    - "h (mm)"                                # æˆªé¢é«˜åº¦
    - "r0 (mm)"                               # åœ†è§’åŠå¾„
    - "t (mm)"                                # é’¢ç®¡åšåº¦
    - "L (mm)"                                # æŸ±é•¿
    - "lambda"                                # é•¿ç»†æ¯”

# æ¨¡å‹é…ç½®
model:
  type: "xgboost"                              # æ¨¡å‹ç±»å‹

  # XGBoostè¶…å‚æ•°
  xgboost_params:
    objective: "reg:squarederror"              # ç›®æ ‡å‡½æ•°
    n_estimators: 200                          # æ ‘çš„æ•°é‡
    learning_rate: 0.1                         # å­¦ä¹ ç‡
    max_depth: 6                               # æ ‘æœ€å¤§æ·±åº¦
    min_child_weight: 3                        # æœ€å°å­èŠ‚ç‚¹æƒé‡
    subsample: 0.8                             # æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
    colsample_bytree: 0.8                      # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    gamma: 0                                   # æœ€å°æŸå¤±å‡å°‘
    reg_alpha: 0                               # L1æ­£åˆ™åŒ–
    reg_lambda: 1                              # L2æ­£åˆ™åŒ–
    max_delta_step: 0                          # æœ€å¤§å¢é‡æ­¥é•¿
    random_state: 42                           # éšæœºç§å­
    n_jobs: -1                                 # å¹¶è¡Œæ•°ï¼ˆ-1ä¸ºå…¨éƒ¨æ ¸å¿ƒï¼‰

# äº¤å‰éªŒè¯é…ç½®
cross_validation:
  k_folds: 5                                   # KæŠ˜äº¤å‰éªŒè¯
  scoring: "neg_root_mean_squared_error"       # è¯„åˆ†æŒ‡æ ‡
  random_state: 42

# è¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptunaï¼‰
optuna:
  enabled: true                                # æ˜¯å¦å¯ç”¨
  n_trials: 100                                # è¯•éªŒæ¬¡æ•°
  timeout: 3600                                # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
  study_name: "cfst_hyperopt"                  # ç ”ç©¶åç§°
  direction: "minimize"                        # ä¼˜åŒ–æ–¹å‘

  # æœç´¢ç©ºé—´
  search_space:
    n_estimators: {"type": "int", "low": 100, "high": 500, "step": 50}
    learning_rate: {"type": "float", "low": 0.01, "high": 0.3, "log": true}
    max_depth: {"type": "int", "low": 3, "high": 10, "step": 1}
    min_child_weight: {"type": "int", "low": 1, "high": 10, "step": 1}
    subsample: {"type": "float", "low": 0.6, "high": 1.0, "step": 0.1}
    colsample_bytree: {"type": "float", "low": 0.6, "high": 1.0, "step": 0.1}
    gamma: {"type": "float", "low": 0, "high": 0.3, "step": 0.05}
    reg_alpha: {"type": "float", "low": 0, "high": 1, "step": 0.1}

# è¾“å‡ºè·¯å¾„
paths:
  output_dir: "output"                         # è¾“å‡ºä¸»ç›®å½•
  model_dir: "output/xgboost_model"            # æ¨¡å‹ä¿å­˜è·¯å¾„
  plots_dir: "output/xgboost_model/plots"      # å›¾è¡¨è·¯å¾„
  prediction_file: "output/predictions.csv"    # é¢„æµ‹ç»“æœæ–‡ä»¶

# æ—¥å¿—é…ç½®
logging:
  level: "INFO"                                # æ—¥å¿—çº§åˆ«
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "output/ml_pipeline.log"               # æ—¥å¿—æ–‡ä»¶
  max_file_size: 10485760                      # å•ä¸ªæ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°ï¼ˆ10MBï¼‰
  backup_count: 5                              # å¤‡ä»½æ•°é‡
  console_output: true                         # æ§åˆ¶å°è¾“å‡º
```

### å…¨é‡ç‰¹å¾é…ç½®ï¼ˆconfig_all_features.yamlï¼‰

æ­¤é…ç½®æ–‡ä»¶**ä¸åŒ…å«åˆ—å‰”é™¤**ï¼Œä½¿ç”¨å…¨éƒ¨24ä¸ªå‚æ•°ï¼Œç”¨äºå¯¹æ¯”åˆ†æã€‚

```yaml
data:
  train_path: "feature_parameters.csv"
  target_column: "Nexp (kN)"
  test_size: 0.2
  random_state: 42
  columns_to_drop: []  # ä¸å‰”é™¤ä»»ä½•åˆ—

# å…¶ä½™é…ç½®ä¸config.yamlç›¸åŒ
...
```

---

## è®­ç»ƒä½¿ç”¨æ‰‹å†Œ

### æ­¥éª¤1ï¼šç¯å¢ƒå‡†å¤‡

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository-url>
cd xgboost

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate     # Windows

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### æ­¥éª¤2ï¼šæ•°æ®å‡†å¤‡

ç¡®ä¿æ•°æ®æ–‡ä»¶ `feature_parameters.csv` æ ¼å¼æ­£ç¡®ï¼š

| b (mm) | h (mm) | t (mm) | ... | Nexp (kN) |
|--------|--------|--------|-----|-----------|
| 100    | 100    | 3.0    | ... | 2500      |
| 120    | 120    | 4.0    | ... | 3200      |

### æ­¥éª¤3ï¼šé…ç½®æ¨¡å‹

ç¼–è¾‘ `config/config.yaml` æ–‡ä»¶ï¼š
- ä¿®æ”¹æ•°æ®è·¯å¾„ï¼ˆå¦‚éœ€ï¼‰
- è°ƒæ•´è¶…å‚æ•°
- é…ç½®äº¤å‰éªŒè¯å‚æ•°

### æ­¥éª¤4ï¼šæ‰§è¡Œè®­ç»ƒ

#### åŸºç¡€è®­ç»ƒï¼ˆå‰”é™¤å‡ ä½•å‚æ•°ï¼‰

```bash
# åŸºæœ¬è®­ç»ƒå‘½ä»¤
python train.py --config config/config.yaml

# è¾“å‡ºç¤ºä¾‹ï¼š
================================================================
          XGBoost ML Pipeline - CFSTæ‰¿è½½åŠ›é¢„æµ‹
================================================================
å‘½ä»¤è¡Œå‚æ•°:
  config: config/config.yaml
  verbose: True

æ­¥éª¤ 1: åŠ è½½æ•°æ®
  è®­ç»ƒæ•°æ®: feature_parameters.csv
  ç›®æ ‡åˆ—: Nexp (kN)
  æ•°æ®é›†å¤§å°: (1000, 24)
  ç‰¹å¾æ•°é‡: 24
  å‰”é™¤åˆ—: ['b (mm)', 'h (mm)', 'r0 (mm)', 't (mm)', 'L (mm)', 'lambda']
  ä¿ç•™ç‰¹å¾: 19

æ­¥éª¤ 2: æ•°æ®é¢„å¤„ç†
  å¡«å……ç¼ºå¤±å€¼ï¼ˆä¸­ä½æ•°æ’è¡¥ï¼‰
  æ‹†åˆ†æ•°æ®é›†: è®­ç»ƒé›†80%, æµ‹è¯•é›†20%

æ­¥éª¤ 3: æ¨¡å‹è®­ç»ƒ
  å¯ç”¨Optunaè¶…å‚æ•°ä¼˜åŒ–
  XGBoostå‚æ•°:
    - n_estimators: 200
    - learning_rate: 0.1
    - max_depth: 6
  è®­ç»ƒæ—¶é—´: 15.23ç§’

æ­¥éª¤ 4: æ¨¡å‹è¯„ä¼°
  RÂ² Score: 0.9876
  RMSE: 254.32 kN
  MAE: 187.45 kN
  MAPE: 8.45%
  COV: 0.0892 (Î¼=0.985, excellent stability)

æ­¥éª¤ 5: å¯è§†åŒ–
  ç”Ÿæˆé¢„æµ‹æ•£ç‚¹å›¾: output/xgboost_model/plots/predictions_scatter.png
  ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾: output/xgboost_model/plots/feature_importance.png

æ­¥éª¤ 6: ä¿å­˜æ¨¡å‹
  æ¨¡å‹ä¿å­˜åˆ°: output/xgboost_model/xgboost_model.pkl
  é¢„å¤„ç†å™¨ä¿å­˜åˆ°: output/xgboost_model/preprocessor.pkl
  è¯„ä¼°æŠ¥å‘Šä¿å­˜åˆ°: output/xgboost_model/evaluation_report.json

è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: 45.32ç§’
================================================================
```

#### å¿«é€Ÿè®­ç»ƒï¼ˆç¦ç”¨Optunaï¼‰

```bash
python train.py --config config/config.yaml --use-optuna false
```

#### å…¨é‡ç‰¹å¾è®­ç»ƒ

```bash
# ä½¿ç”¨åŒ…å«å‡ ä½•å‚æ•°çš„æ•°æ®
python train.py --config config/config_all_features.yaml
```

### æ­¥éª¤5ï¼šæŸ¥çœ‹è®­ç»ƒç»“æœ

è®­ç»ƒå®Œæˆåï¼ŒæŸ¥çœ‹è¾“å‡ºç›®å½•ï¼š

```bash
tree output/xgboost_model/
```

è¾“å‡ºç›®å½•ç»“æ„ï¼š
```
output/xgboost_model/
â”œâ”€â”€ xgboost_model.pkl           # è®­ç»ƒå¥½çš„æ¨¡å‹
â”œâ”€â”€ preprocessor.pkl            # é¢„å¤„ç†å™¨
â”œâ”€â”€ evaluation_report.json      # è¯„ä¼°æŠ¥å‘Š
â”œâ”€â”€ feature_names.json          # ç‰¹å¾åç§°åˆ—è¡¨
â””â”€â”€ plots/
    â”œâ”€â”€ predictions_scatter.png # é¢„æµ‹vsçœŸå®æ•£ç‚¹å›¾
    â”œâ”€â”€ predictions_residual.png # æ®‹å·®å›¾
    â””â”€â”€ feature_importance.png  # ç‰¹å¾é‡è¦æ€§å›¾
```

### æ­¥éª¤6ï¼šè§£è¯»è¯„ä¼°æŠ¥å‘Š

æ‰“å¼€ `output/xgboost_model/evaluation_report.json`ï¼š

```json
{
    "model_info": {
        "type": "xgboost",
        "trained_at": "2026-01-14T10:30:00",
        "training_time": 45.32
    },
    "test_metrics": {
        "r2_score": 0.9876,
        "rmse": 254.32,
        "mae": 187.45,
        "mape": 8.45,
        "cov": 0.0892,
        "cov_mean_ratio": 0.985
    },
    "cv_metrics": {
        "mean_rmse": 267.85,
        "std_rmse": 23.45
    }
}
```

**å…³é”®æŒ‡æ ‡è§£è¯»**ï¼š
- **RÂ² (0.9876)**ï¼šä¼˜ç§€ï¼Œè§£é‡Š98.76%çš„æ–¹å·®
- **RMSE (254.32)**ï¼šå¹³å‡é¢„æµ‹è¯¯å·®254.32kN
- **COV (0.0892)**ï¼šä¼˜ç§€ï¼Œé¢„æµ‹ç¨³å®šæ€§å¾ˆå¥½ï¼ˆ<0.10ï¼‰
- **COV Mean Ratio (0.985)**ï¼šæ¥è¿‘1.0ï¼Œæ— ç³»ç»Ÿæ€§åå·®

---

## é¢„æµ‹ä½¿ç”¨æ‰‹å†Œ

### æ–¹å¼1ï¼šæ‰¹é‡é¢„æµ‹ï¼ˆCSVæ–‡ä»¶ï¼‰

```bash
python predict.py --model output/xgboost_model --input all.csv --output predictions.csv
```

å‚æ•°è¯´æ˜ï¼š
- `--model`ï¼šæ¨¡å‹ç›®å½•è·¯å¾„
- `--input`ï¼šè¾“å…¥CSVæ–‡ä»¶è·¯å¾„
- `--output`ï¼šè¾“å‡ºé¢„æµ‹ç»“æœæ–‡ä»¶è·¯å¾„
- `--batch-size`ï¼šæ‰¹é‡å¤§å°ï¼ˆé»˜è®¤1000ï¼‰

è¾“å‡ºCSVæ ¼å¼ï¼š
```csv
index,prediction_Nexp (kN)
0,2500.5
1,3200.2
2,2850.7
...
```

### æ–¹å¼2ï¼šå•æ¡é¢„æµ‹ï¼ˆäº¤äº’å¼ï¼‰

```bash
python predict.py --model output/xgboost_model --single
```

äº¤äº’è¾“å…¥ç¤ºä¾‹ï¼š
```
è¯·è¾“å…¥é¢„æµ‹å‚æ•°ï¼ˆæ ¼å¼ï¼škey=valueï¼Œæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š
fc (MPa)=40.5
fy (MPa)=350.2
Ac (mm^2)=10000
As (mm^2)=500
...

é¢„æµ‹ç»“æœï¼š
æ‰¿è½½åŠ›: 2850.7 kN
ç½®ä¿¡åŒºé—´: [2720.5, 2980.9] kN
COV: 0.089 (ä¼˜ç§€)
```

### æ–¹å¼3ï¼šPython APIè°ƒç”¨

```python
from src.predictor import Predictor

# åˆå§‹åŒ–é¢„æµ‹å™¨
predictor = Predictor("output/xgboost_model")

# å•æ¡é¢„æµ‹
input_data = {
    "fc (MPa)": 40.5,
    "fy (MPa)": 350.2,
    "Ac (mm^2)": 10000,
    # ... å…¶ä»–å‚æ•°
}
result = predictor.predict_single(input_data)
print(f"é¢„æµ‹æ‰¿è½½åŠ›: {result['prediction']:.2f} kN")
print(f"COV: {result['cov']:.4f}")

# æ‰¹é‡é¢„æµ‹
import pandas as pd
df = pd.read_csv("input_data.csv")
predictions = predictor.predict_batch(df, save_to="batch_predictions.csv")
```

### é¢„æµ‹ç»“æœè§£è¯»

é¢„æµ‹è¾“å‡ºåŒ…å«ï¼š
- **prediction**: é¢„æµ‹æ‰¿è½½åŠ›ï¼ˆkNï¼‰
- **confidence_interval**: 95%ç½®ä¿¡åŒºé—´ï¼ˆåŸºäºCOVè®¡ç®—ï¼‰
- **cov**: å˜å¼‚ç³»æ•°ï¼Œè¯„ä¼°é¢„æµ‹ç¨³å®šæ€§

**COVç­‰çº§**ï¼š
- **< 0.05**: æå¥½
- **0.05-0.10**: ä¼˜ç§€ï¼ˆæ¨èéƒ¨ç½²é˜ˆå€¼ï¼‰
- **0.10-0.15**: è‰¯å¥½
- **0.15-0.20**: å¯æ¥å—
- **> 0.20**: ä¸ç¨³å®š

---

## ç‰¹å¾é€‰æ‹©ç®¡é“ä½¿ç”¨æ‰‹å†Œ

### ä¸ºä»€ä¹ˆéœ€è¦ç‰¹å¾é€‰æ‹©ï¼Ÿ

ç‰¹å¾é€‰æ‹©ç®¡é“é€šè¿‡è¿­ä»£å‰”é™¤æœ€ä¸é‡è¦çš„ç‰¹å¾ï¼Œå¸®åŠ©æ‰¾åˆ°ï¼š
- **æœ€ä¼˜ç‰¹å¾å­é›†**ï¼šåœ¨å‡†ç¡®æ€§å’Œå¤æ‚åº¦é—´å¹³è¡¡
- **å…³é”®å½±å“å› ç´ **ï¼šè¯†åˆ«æœ€é‡è¦çš„å·¥ç¨‹å‚æ•°
- **æ¨¡å‹ç®€åŒ–**ï¼šå‡å°‘è¿‡æ‹Ÿåˆé£é™©ï¼Œæé«˜é¢„æµ‹æ•ˆç‡

### å¿«é€Ÿå¼€å§‹

#### æ–¹å¼1ï¼šå‰”é™¤å‡ ä½•å‚æ•°ï¼ˆæ¨èï¼‰

```bash
python feature_selection_pipeline.py --config config/config.yaml
```

#### æ–¹å¼2ï¼šå…¨é‡ç‰¹å¾é€‰æ‹©

```bash
python feature_selection_pipeline.py --config config/config_all_features.yaml
```

### é«˜çº§é…ç½®

```bash
# æŒ‡å®šè¾“å‡ºç›®å½•
python feature_selection_pipeline.py --config config/config.yaml --output-dir output/my_feature_selection

# æŒ‡å®šæœ€å¤§ç‰¹å¾æ•°ï¼ˆè‡ªåŠ¨åœæ­¢ï¼‰
python feature_selection_pipeline.py --config config/config.yaml --max-features 15

# è°ƒæ•´äº¤å‰éªŒè¯æŠ˜æ•°
python feature_selection_pipeline.py --config config/config.yaml --cv-folds 10
```

### ç®¡é“æ‰§è¡Œæµç¨‹

```
å¼€å§‹
  â†“
åŠ è½½é…ç½®å’Œæ•°æ®
  â†“
åˆå§‹åŒ–ï¼šä½¿ç”¨å…¨éƒ¨ç‰¹å¾è®­ç»ƒæ¨¡å‹
  â†“
è®°å½•æ€§èƒ½æŒ‡æ ‡ï¼ˆRÂ², COVç­‰ï¼‰
  â†“
è·å–ç‰¹å¾é‡è¦æ€§æ’åº
  â†“
å‰”é™¤é‡è¦æ€§æœ€ä½ç‰¹å¾ï¼ˆ1ä¸ªï¼‰
  â†“
ç”¨å‰©ä½™ç‰¹å¾é‡æ–°è®­ç»ƒ
  â†“
é‡å¤è¿­ä»£ç›´åˆ°åªå‰©1ä¸ªç‰¹å¾
  â†“
ç”Ÿæˆæ€§èƒ½æ›²çº¿å›¾
  â†“
è¯†åˆ«æœ€ä¼˜å­é›†ï¼ˆBest RÂ², Best CV, Elbowï¼‰
  â†“
è¾“å‡ºæ€»ç»“æŠ¥å‘Š
  â†“
ç»“æŸ
```

### è¾“å‡ºç»“æœè§£è¯»

ç‰¹å¾é€‰æ‹©å®Œæˆåï¼ŒæŸ¥çœ‹è¾“å‡ºç›®å½•ï¼š

```bash
tree output/feature_selection/
```

è¾“å‡ºå†…å®¹ï¼š

```
output/feature_selection/
â”œâ”€â”€ feature_selection_results.csv   # æ¯æ¬¡è¿­ä»£çš„æ€§èƒ½è®°å½•
â”œâ”€â”€ detailed_results.json           # è¯¦ç»†ç»“æœï¼ˆJSONï¼‰
â”œâ”€â”€ feature_selection_curves.png    # æ€§èƒ½æ›²çº¿å›¾ï¼ˆå¸¦COVï¼‰
â”œâ”€â”€ performance_summary.png         # æ€§èƒ½å¯¹æ¯”å›¾
â”œâ”€â”€ summary_report.txt              # æ€»ç»“æŠ¥å‘Š
â”œâ”€â”€ optimal_subsets.json            # æœ€ä¼˜å­é›†é…ç½®
â””â”€â”€ iteration_001-iteration_018/    # æ¯ä¸ªè¿­ä»£çš„è¯¦ç»†ç»“æœ
    â”œâ”€â”€ model.pkl
    â”œâ”€â”€ evaluation_report.json
    â””â”€â”€ plots/
```

### æ€§èƒ½æ›²çº¿åˆ†æ

ç‰¹å¾é€‰æ‹©æ›²çº¿å›¾ï¼ˆ5ä¸ªå­å›¾ï¼‰ï¼š

1. **RÂ² vs ç‰¹å¾æ•°é‡**ï¼šè¯†åˆ«æœ€ä½³RÂ²å­é›†
2. **RMSE vs ç‰¹å¾æ•°é‡**ï¼šè¯„ä¼°é¢„æµ‹è¯¯å·®
3. **æœ€ä¼˜CV vs ç‰¹å¾æ•°é‡**ï¼šäº¤å‰éªŒè¯ä¼˜åŒ–
4. **ç‰¹å¾é‡è¦æ€§å æ¯”**ï¼šç´¯ç§¯é‡è¦æ€§
5. **COV vs ç‰¹å¾æ•°é‡**ï¼šğŸ”¥ æ–°å¢-è¯„ä¼°é¢„æµ‹ç¨³å®šæ€§

### æœ€ä¼˜å­é›†æ¨è

æŸ¥çœ‹ `summary_report.txt`ï¼š

```
FEATURE SELECTION SUMMARY REPORT
================================

è¿­ä»£æ¬¡æ•°: 18
åˆå§‹ç‰¹å¾æ•°: 18
æœ€ç»ˆç‰¹å¾æ•°: 1

OPTIMAL FEATURE SUBSETS:
-----------------------

1. Best RÂ² Score:                    # RÂ²æœ€ä¼˜
   - Iteration: 2
   - Features: 17                    # 17ä¸ªç‰¹å¾
   - RÂ² Score: 0.9964                # RÂ²=0.9964
   - RMSE: 204.24                    # RMSE=204.24
   - COV: 0.1049                     # COV=0.1049ï¼ˆä¼˜ç§€ï¼‰
   - Feature list: fy, fc, ...

2. Best Cross-Validation:            # äº¤å‰éªŒè¯æœ€ä¼˜
   - Iteration: 2
   - Features: 17
   - CV RMSE: -630.42                # è´ŸRMSE
   - RÂ²: 0.9964
   - COV: 0.1049
   - Feature list: fy, fc, ...

3. Elbow Method:                     # è‚˜éƒ¨æ³•
   - Iteration: 11
   - Features: 8                     # ä»…8ä¸ªå…³é”®ç‰¹å¾
   - RÂ² Score: 0.9916                # ä¿æŒ0.9916
   - COV: 0.1629                     # ç¨³å®šæ€§å¯æ¥å—
   - Improvement Rate: 0.0002        # æå‡å·²å¹³ç¼“
   - Feature list: Ac, As, Re, ...

RECOMMENDATIONS:
---------------
- If maximizing accuracy: Use Best RÂ² subset (17 features)
- If balancing accuracy and simplicity: Use Elbow (8 features)
- For production: Consider Best CV for generalization

COV INTERPRETATION:
------------------
- COV < 0.10: æå¥½
- COV < 0.15: è‰¯å¥½
- COV < 0.20: å¯æ¥å—
```

### å¦‚ä½•é€‰æ‹©æœ€ä¼˜å­é›†ï¼Ÿ

**å·¥ç¨‹å®è·µå»ºè®®**ï¼š

1. **æœ€ä½³å‡†ç¡®åº¦**ï¼šé€‰æ‹©**Best RÂ²**ï¼ˆ17ä¸ªç‰¹å¾ï¼‰
   - RÂ² = 0.9964ï¼ˆå‡ ä¹æ— æ€§èƒ½æŸå¤±ï¼‰
   - COV = 0.1049ï¼ˆä¼˜ç§€ç¨³å®šæ€§ï¼‰

2. **å¹³è¡¡é€‰æ‹©**ï¼šé€‰æ‹©**Elbow Method**ï¼ˆ8ä¸ªç‰¹å¾ï¼‰
   - RÂ² = 0.9916ï¼ˆä»…æŸå¤±0.5%ï¼‰
   - COV = 0.1629ï¼ˆè‰¯å¥½ç¨³å®šæ€§ï¼‰
   - **å¤æ‚åº¦é™ä½55.6%**

3. **ç”Ÿäº§éƒ¨ç½²**ï¼šé€‰æ‹©**Best CV**ï¼ˆ17ä¸ªç‰¹å¾ï¼‰
   - äº¤å‰éªŒè¯æœ€ä¼˜ï¼Œæ³›åŒ–èƒ½åŠ›æœ€å¥½
   - COV = 0.1049

### å¯¹æ¯”åˆ†æ

ä½¿ç”¨å¯¹æ¯”é…ç½®æ–‡ä»¶æŸ¥çœ‹ä¸¤ç§ç­–ç•¥çš„å·®å¼‚ï¼š

```bash
# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
cat output/feature_selection_comparison.csv
```

å¯¹æ¯”ç»“æœè§£è¯»ï¼š

| æŒ‡æ ‡           | æ— å‡ ä½•å‚æ•° | æœ‰å‡ ä½•å‚æ•° | è¯´æ˜                 |
| -------------- | ---------- | ---------- | -------------------- |
| åˆå§‹ç‰¹å¾æ•°     | 18         | 24         | å‰”é™¤6ä¸ªå‡ ä½•å‚æ•°      |
| Best RÂ²        | 0.9964     | 0.9965     | **å‡ ä¹æ— æ€§èƒ½æŸå¤±**   |
| Bestç‰¹å¾æ•°     | 17         | 21         | æœ€ä¼˜å­é›†å¤§å°         |
| Best RMSE      | 204.24     | 200.53     | è¯¯å·®ç›¸è¿‘             |
| **Best COV**   | **0.1049** | **0.1116** | **æ— å‡ ä½•å‚æ•°æ›´å¥½**   |

**æ ¸å¿ƒå‘ç°**ï¼šå‰”é™¤å‡ ä½•å‚æ•°åï¼Œä»…ç”¨18ä¸ªæ— é‡çº²å‚æ•°å³å¯è¾¾åˆ°ç›¸åŒç²¾åº¦ï¼Œè¯æ˜å¤šæˆªé¢ç»Ÿä¸€é¢„æµ‹æ¨¡å‹å¯è¡Œï¼

---

## è¯¦ç»†å‚æ•°è¯´æ˜

### ä¿ç•™çš„18ä¸ªæ— é‡çº²å‚æ•°

| å‚æ•°å          | å•ä½ | ç‰©ç†æ„ä¹‰                     | å·¥ç¨‹æ„ä¹‰             |
| --------------- | ---- | ---------------------------- | -------------------- |
| R               | %    | å†ç”Ÿç²—éª¨æ–™å–ä»£ç‡             | ç¯ä¿ææ–™æ¯”ä¾‹         |
| fy              | MPa  | é’¢æå±ˆæœå¼ºåº¦                 | é’¢æå¼ºåº¦             |
| fc              | MPa  | æ··å‡åœŸæŠ—å‹å¼ºåº¦               | æ··å‡åœŸå¼ºåº¦           |
| e1              | mm   | ä¸Šç«¯åå¿ƒè·                   | ä¸Šéƒ¨åå¿ƒ             |
| e2              | mm   | ä¸‹ç«¯åå¿ƒè·                   | ä¸‹éƒ¨åå¿ƒ             |
| r0/h            | -    | è§’å¾„æ¯”                       | æˆªé¢å½¢çŠ¶             |
| b/t             | -    | å¾„åšæ¯”                       | ç®¡å£ç›¸å¯¹åšåº¦         |
| Ac              | mmÂ²  | ç­‰æ•ˆæ ¸å¿ƒæ··å‡åœŸé¢ç§¯           | æ··å‡åœŸé¢ç§¯           |
| As              | mmÂ²  | ç­‰æ•ˆé’¢ç®¡é¢ç§¯                 | é’¢æé¢ç§¯             |
| Re              | mm   | ç­‰æ•ˆæ··å‡åœŸåŠå¾„               | ç­‰æ•ˆåŠå¾„             |
| te              | mm   | ç­‰æ•ˆé’¢ç®¡åšåº¦                 | ç­‰æ•ˆåšåº¦             |
| ke              | -    | çº¦æŸæœ‰æ•ˆæ€§ç³»æ•°               | çº¦æŸæ•ˆæœ             |
| xi              | -    | å¥—ç®ç³»æ•°                     | å¥—ç®ä½œç”¨             |
| sigma_re        | MPa  | æœ‰æ•ˆä¾§å‘åº”åŠ›                 | çº¦æŸåº”åŠ›             |
| lambda_bar      | -    | ç›¸å¯¹é•¿ç»†æ¯”                   | é•¿ç»†æ¯”               |
| e/h             | -    | æœ€å¤§åå¿ƒç‡                   | åå¿ƒç¨‹åº¦             |
| e1/e2           | -    | åå¿ƒæ¯”ç‡                     | ä¸Šä¸‹åå¿ƒæ¯”           |
| e_bar           | -    | ç›¸å¯¹åå¿ƒç‡                   | ç›¸å¯¹åå¿ƒ             |

### è¢«å‰”é™¤çš„6ä¸ªå‡ ä½•å‚æ•°

è¿™äº›å‚æ•°å› ä¸æ‰¿è½½èƒ½åŠ›éçº¿æ€§å…³ç³»ï¼Œè¢«å‰”é™¤ä»¥å®ç°å¤šæˆªé¢ç»Ÿä¸€é¢„æµ‹ï¼š

- **b (mm)**: æˆªé¢å®½åº¦
- **h (mm)**: æˆªé¢é«˜åº¦
- **r0 (mm)**: åœ†è§’åŠå¾„
- **t (mm)**: é’¢ç®¡åšåº¦
- **L (mm)**: æŸ±é•¿
- **lambda**: é•¿ç»†æ¯”ï¼ˆåŸå§‹å€¼ï¼‰

---

## COVï¼ˆå˜å¼‚ç³»æ•°ï¼‰è¯¦è§£

### COVè®¡ç®—åŸç†

```python
def calculate_cov(y_true, y_pred):
    """
    è®¡ç®—å˜å¼‚ç³»æ•°ï¼ˆCoefficient of Variationï¼‰

    æ­¥éª¤ï¼š
    1. è®¡ç®—é¢„æµ‹/çœŸå®æ¯”å€¼ï¼šÎ¾_i = y_pred_i / y_true_i
    2. è®¡ç®—å‡å€¼ï¼šÎ¼ = mean(Î¾_i)
    3. è®¡ç®—æ ‡å‡†å·®ï¼šÏƒ = std(Î¾_i)
    4. COV = Ïƒ / Î¼
    """
    ratios = y_pred / y_true
    mean_ratio = np.mean(ratios)
    std_ratio = np.std(ratios)
    cov = std_ratio / mean_ratio
    return cov
```

### COVç‰©ç†æ„ä¹‰

- **Î¼ â‰ˆ 1.0**: æ— ç³»ç»Ÿæ€§åå·®ï¼ˆç†æƒ³çŠ¶æ€ï¼‰
- **COV < 0.10**: æå¥½çš„é¢„æµ‹ç¨³å®šæ€§ï¼ˆæ¨èéƒ¨ç½²é˜ˆå€¼ï¼‰
- **Î¾ > 1.0**: é¢„æµ‹å€¼å¤§äºçœŸå®å€¼ï¼ˆå¯èƒ½ä¸å®‰å…¨ï¼‰
- **Î¾ < 1.0**: é¢„æµ‹å€¼å°äºçœŸå®å€¼ï¼ˆåä¿å®ˆ/å®‰å…¨ï¼‰

### å·¥ç¨‹æ ‡å‡†

åœ¨ç»“æ„å·¥ç¨‹ä¸­ï¼š
- **COV < 0.10**: ä¼˜ç§€æ¨¡å‹ï¼Œå¯ç”¨äºç”Ÿäº§
- **COV < 0.15**: è‰¯å¥½æ¨¡å‹ï¼Œéœ€å¢åŠ å®‰å…¨è£•åº¦
- **COV > 0.20**: ä¸ç¨³å®šï¼Œä¸å»ºè®®ä½¿ç”¨

---

## å¸¸è§é—®é¢˜è§£ç­”

### Q1: æ¨¡å‹è®­ç»ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ

**A**: å–å†³äºæ•°æ®é‡å’Œé…ç½®ï¼š
- æ— Optunaï¼ˆé»˜è®¤å‚æ•°ï¼‰ï¼š~30ç§’
- å¯ç”¨Optunaï¼ˆ100æ¬¡è¯•éªŒï¼‰ï¼š~15-30åˆ†é’Ÿ
- ç‰¹å¾é€‰æ‹©ï¼ˆ18æ¬¡è¿­ä»£ï¼‰ï¼š~5-10åˆ†é’Ÿ

### Q2: å¦‚ä½•å¤„ç†ç¼ºå¤±å€¼ï¼Ÿ

**A**: é¢„å¤„ç†å™¨è‡ªåŠ¨ä½¿ç”¨ä¸­ä½æ•°å¡«å……ï¼š
```python
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')
```

### Q3: æ¨¡å‹æ–‡ä»¶å¯ä»¥åœ¨å…¶ä»–è®¡ç®—æœºä½¿ç”¨å—ï¼Ÿ

**A**: å¯ä»¥ï¼Œéœ€æ»¡è¶³ï¼š
1. ç›¸åŒçš„Pythonç‰ˆæœ¬
2. å®‰è£…ç›¸åŒçš„ä¾èµ–ï¼ˆrequirements.txtï¼‰
3. ç›¸åŒçš„æ•°æ®æ ¼å¼ï¼ˆç‰¹å¾åç§°ä¸€è‡´ï¼‰

### Q4: å¦‚ä½•è°ƒæ•´æ¨¡å‹å¤æ‚åº¦ï¼Ÿ

**A**: ä¿®æ”¹ `config/config.yaml`ï¼š
```yaml
model:
  xgboost_params:
    n_estimators: 100    # å‡å°‘æ ‘çš„æ•°é‡
    max_depth: 4         # é™åˆ¶æ ‘æ·±åº¦
    min_child_weight: 5  # å¢åŠ æœ€å°å¶å­æƒé‡
```

### Q5: ç‰¹å¾é€‰æ‹©åå¦‚ä½•é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Ÿ

**A**: ä½¿ç”¨æœ€ä¼˜å­é›†é‡æ–°è®­ç»ƒï¼š

```python
# ä»JSONè¯»å–æœ€ä¼˜ç‰¹å¾
import json
with open("output/feature_selection/optimal_subsets.json", "r") as f:
    subsets = json.load(f)

best_features = subsets["best_r2"]["features"]

# é‡æ–°è®­ç»ƒï¼ˆå‚è€ƒ train.py å®ç°ï¼‰
```

### Q6: å¦‚ä½•è§£é‡Šç‰¹å¾é‡è¦æ€§ï¼Ÿ

**A**: XGBoostç‰¹å¾é‡è¦æ€§åŸºäºï¼š
1. **Gain**: ç‰¹å¾åœ¨åˆ†è£‚ä¸­çš„å¹³å‡å¢ç›Š
2. **Frequency**: ä½œä¸ºåˆ†è£‚ç‰¹å¾çš„æ¬¡æ•°
3. **Cover**: è¦†ç›–çš„æ ·æœ¬æ¯”ä¾‹

åœ¨CFSTä¸­ï¼Œé€šå¸¸æœ€é‡è¦çš„ç‰¹å¾ï¼š
- `fc` (æ··å‡åœŸå¼ºåº¦)
- `fy` (é’¢æå¼ºåº¦)
- `lambda_bar` (é•¿ç»†æ¯”)
- `e/h` (åå¿ƒç‡)

---

## æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°ç‰¹å¾

1. **ä¿®æ”¹æ•°æ®æ–‡ä»¶**ï¼šåœ¨ `feature_parameters.csv` æ·»åŠ æ–°åˆ—
2. **æ›´æ–°é…ç½®æ–‡ä»¶**ï¼šå¦‚æœä¸éœ€è¦å‰”é™¤ï¼Œæ— éœ€ä¿®æ”¹
3. **é‡æ–°è®­ç»ƒ**ï¼šæ‰§è¡Œè®­ç»ƒå‘½ä»¤
4. **éªŒè¯**ï¼šæ£€æŸ¥æ¨¡å‹æ€§èƒ½æå‡

### æ›´æ¢æ¨¡å‹ç®—æ³•

ç›®å‰ä»…æ”¯æŒXGBoostï¼Œå¦‚éœ€æ›´æ¢ï¼š

```python
# ä¿®æ”¹ src/model_trainer.py
class ModelTrainer:
    def __init__(self, config):
        # æ›´æ¢ä¸ºå…¶ä»–æ¨¡å‹
        from sklearn.ensemble import RandomForestRegressor
        self.model = RandomForestRegressor(**config)
```

### è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

æ·»åŠ æ–°æŒ‡æ ‡åˆ° `src/evaluator.py`ï¼š

```python
def calculate_custom_metric(self, y_true, y_pred):
    """è‡ªå®šä¹‰æŒ‡æ ‡"""
    # å®ç°è‡ªå®šä¹‰é€»è¾‘
    return metric_value
```

### é›†æˆåˆ°WebæœåŠ¡

```python
from flask import Flask, request, jsonify
from src.predictor import Predictor

app = Flask(__name__)
predictor = Predictor("output/xgboost_model")

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    result = predictor.predict_single(data)
    return jsonify(result)

if __name__ == '__main__':
    app.run(port=5000)
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–

1. **å‡å°‘Optunaè¯•éªŒæ¬¡æ•°**ï¼š
   ```yaml
   optuna:
     n_trials: 50  # ä»100å‡å°‘åˆ°50
   ```

2. **å‡å°‘äº¤å‰éªŒè¯æŠ˜æ•°**ï¼š
   ```yaml
   cross_validation:
     k_folds: 3  # ä»5å‡å°‘åˆ°3
   ```

3. **å¯ç”¨GPUåŠ é€Ÿ**ï¼š
   ```yaml
   model:
     xgboost_params:
       tree_method: "gpu_hist"  # ä½¿ç”¨GPU
   ```

### é¢„æµ‹é€Ÿåº¦ä¼˜åŒ–

1. **æ‰¹é‡é¢„æµ‹**ï¼š
   ```python
   # é¿å…é€æ¡é¢„æµ‹
   predictions = predictor.predict_batch(df)  # å¿«

   # é¿å…
   for index, row in df.iterrows():
       predictor.predict_single(row.to_dict())  # æ…¢
   ```

2. **æ¨¡å‹é‡åŒ–**ï¼šä½¿ç”¨XGBoostçš„æ¨¡å‹å‹ç¼©åŠŸèƒ½

3. **ç‰¹å¾ç¼“å­˜**ï¼šå¯¹é‡å¤é¢„æµ‹çš„æ•°æ®ç¼“å­˜é¢„å¤„ç†ç»“æœ

---

## è®¸å¯è¯

MIT License

---

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»ï¼š
- é¡¹ç›®ç»´æŠ¤è€…ï¼šYour Name
- é‚®ç®±ï¼šyour.email@example.com

---

## ç‰ˆæœ¬å†å²

### v1.0.0 (2026-01-14)
- âœ… å®Œæ•´XGBoost MLç®¡é“
- âœ… ç‰¹å¾é€‰æ‹©ç®¡é“
- âœ… COVå˜å¼‚ç³»æ•°é›†æˆ
- âœ… å¯¹æ¯”åˆ†ææŠ¥å‘Š
- âœ… å…¨é¢æ–‡æ¡£

### v0.9.0 (2026-01-13)
- âœ… åŸºç¡€è®­ç»ƒå’Œé¢„æµ‹åŠŸèƒ½
- âœ… æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
- âœ… æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–

---

**æ–‡æ¡£æœ€åæ›´æ–°**: 2026-01-14
**é¡¹ç›®çŠ¶æ€**: âœ… å·²å®Œæˆæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
