# CFSTæŸ±æé™æ‰¿è½½åŠ›é¢„æµ‹ - XGBoost MLç®¡é“

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ ç®¡é“ï¼Œç”¨äºé¢„æµ‹æ··å‡åœŸå¡«å……é’¢ç®¡ï¼ˆCFSTï¼‰æŸ±çš„æé™æ‰¿è½½åŠ›ã€‚é‡‡ç”¨XGBoostç®—æ³•ï¼Œæä¾›ä»æ•°æ®åŠ è½½ã€é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°åˆ°é¢„æµ‹çš„å…¨æµç¨‹è§£å†³æ–¹æ¡ˆã€‚

### æ ¸å¿ƒç‰¹æ€§

- **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¸…æ™°çš„æ¨¡å—åˆ†ç¦»ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
- **ç‰¹å¾å·¥ç¨‹æ”¯æŒ**ï¼šå¤„ç†æ— é‡çº²å‚æ•°ï¼Œå‰”é™¤å‡ ä½•å‚æ•°å®ç°å¤šæˆªé¢ç»Ÿä¸€é¢„æµ‹
- **è‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–**ï¼šé›†æˆOptunaæ¡†æ¶è¿›è¡Œè¶…å‚æ•°æœç´¢ï¼Œæ”¯æŒæŒä¹…åŒ–å­˜å‚¨å’Œæ–­ç‚¹ç»­è®­
- **æ™ºèƒ½å‚æ•°åŠ è½½**ï¼šè‡ªåŠ¨åŠ è½½æœ€ä¼˜å‚æ•°ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®
- **ç‰¹å¾é€‰æ‹©**ï¼šè¿­ä»£å‰”é™¤æ³•è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜ç‰¹å¾å­é›†
- **å¤šç»´åº¦è¯„ä¼°**ï¼šé›†æˆRÂ²ã€RMSEã€MAEã€MAPEã€COVç­‰å·¥ç¨‹æŒ‡æ ‡
- **å¯è§†åŒ–åˆ†æ**ï¼šæä¾›é¢„æµ‹æ•£ç‚¹å›¾ã€ç‰¹å¾é‡è¦æ€§å›¾å’Œå­¦ä¹ æ›²çº¿
- **COVç¨³å®šæ€§æŒ‡æ ‡**ï¼šç‰¹åˆ«æ·»åŠ å˜å¼‚ç³»æ•°è¯„ä¼°æ¨¡å‹é¢„æµ‹ç¨³å®šæ€§

---

## ç¯å¢ƒè¦æ±‚

### ç³»ç»Ÿè¦æ±‚
- Python 3.8+
- 8GB+ RAMï¼ˆæ¨è16GBï¼‰
- 5GB+ å¯ç”¨ç£ç›˜ç©ºé—´

### Pythonä¾èµ–
- pandas &gt;= 1.5.0
- numpy &gt;= 1.23.0
- xgboost &gt;= 1.7.0
- scikit-learn &gt;= 1.2.0
- matplotlib &gt;= 3.6.0
- seaborn &gt;= 0.12.0
- optuna &gt;= 3.0.0
- joblib &gt;= 1.2.0

---

## é¡¹ç›®ç»“æ„

```
xgboost/
â”œâ”€â”€ config/                           # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â””â”€â”€ config.yaml                   # ä¸»é…ç½®ï¼ˆé€šè¿‡use_optunaåˆ‡æ¢è®­ç»ƒé˜¶æ®µï¼‰
â”œâ”€â”€ data/                             # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/                          # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/                    # å¤„ç†åæ•°æ®
â”‚   â””â”€â”€ models/                       # è®­ç»ƒå¥½çš„æ¨¡å‹
â”œâ”€â”€ logs/                             # æ—¥å¿—å’Œä¼˜åŒ–æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ optuna_study.db               # OptunaæŒä¹…åŒ–å­˜å‚¨ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
â”‚   â””â”€â”€ best_params.json              # æœ€ä¼˜å‚æ•°è®°å½•ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
â”œâ”€â”€ output/                           # è¾“å‡ºç»“æœç›®å½•
â”‚   â”œâ”€â”€ xgboost_model/                # æ¨¡å‹è¾“å‡º
â”‚   â”œâ”€â”€ feature_selection/            # ç‰¹å¾é€‰æ‹©ç»“æœï¼ˆ18å‚æ•°ï¼‰
â”‚   â”œâ”€â”€ feature_selection_all/        # ç‰¹å¾é€‰æ‹©ç»“æœï¼ˆå«å‡ ä½•å‚æ•°ï¼‰
â”‚   â”œâ”€â”€ feature_selection_with_cov/   # å¸¦COVçš„ç‰¹å¾é€‰æ‹©
â”‚   â””â”€â”€ feature_selection_comparison.csv
â”œâ”€â”€ src/                              # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ data_loader.py                # æ•°æ®åŠ è½½æ¨¡å—
â”‚   â”œâ”€â”€ preprocessor.py               # æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ model_trainer.py              # æ¨¡å‹è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ evaluator.py                  # æ¨¡å‹è¯„ä¼°å™¨
â”‚   â”œâ”€â”€ visualizer.py                 # å¯è§†åŒ–æ¨¡å—
â”‚   â”œâ”€â”€ predictor.py                  # é¢„æµ‹æ¨¡å—
â”‚   â””â”€â”€ utils/                        # å·¥å…·æ¨¡å—
â”œâ”€â”€ train.py                          # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ predict.py                        # é¢„æµ‹è„šæœ¬
â”œâ”€â”€ feature_selection_pipeline.py     # ç‰¹å¾é€‰æ‹©ç®¡é“
â”œâ”€â”€ requirements.txt                  # Pythonä¾èµ–
â””â”€â”€ README.md                         # æœ¬æ–‡æ¡£
```

---

## æ•´ä½“æ¶æ„æµç¨‹å›¾

```mermaid
graph TD
    A[åŸå§‹æ•°æ®] --> B[æ•°æ®åŠ è½½å™¨<br/>DataLoader]
    B --> C[å‚æ•°æ£€æŸ¥]
    C --> D[æ•°æ®åˆ†å‰²<br/>train/test]
    D --> E[é¢„å¤„ç†å™¨<br/>Preprocessor]
    E --> F[åˆ—å‰”é™¤<br/>å‰”é™¤å‡ ä½•å‚æ•°]
    F --> G[ç¼ºå¤±å€¼å¡«å……<br/>ä¸­ä½æ•°æ’è¡¥]
    G --> H[æ¨¡å‹è®­ç»ƒå™¨<br/>ModelTrainer]
    H --> I[è¶…å‚æ•°ä¼˜åŒ–<br/>Optuna]
    I --> J[XGBoostè®­ç»ƒ]
    J --> K[æ¨¡å‹ä¿å­˜<br/>model.pkl]
    K --> L[è¯„ä¼°å™¨<br/>Evaluator]
    L --> M[å¤šæŒ‡æ ‡è®¡ç®—<br/>RÂ²/RMSE/COV]
    M --> N[å¯è§†åŒ–å™¨<br/>Visualizer]
    N --> O[ç”ŸæˆæŠ¥å‘Š<br/>å›¾ç‰‡/JSON]
    O --> P[é¢„æµ‹å™¨<br/>Predictor]
    P --> Q[æ‰¹é‡é¢„æµ‹<br/>CSVè¾“å‡º]

    subgraph ç‰¹å¾é€‰æ‹©ç®¡é“
        K --> R[è¿­ä»£è®­ç»ƒ]
        R --> S[å‰”é™¤é‡è¦æ€§æœ€ä½ç‰¹å¾]
        S --> T[æ€§èƒ½æ›²çº¿]
        T --> U[æœ€ä¼˜å­é›†é€‰æ‹©]
    end
```

---

## ç³»ç»Ÿæ—¶åºå›¾

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Train as train.py
    participant Loader as DataLoader
    participant Prep as Preprocessor
    participant Trainer as ModelTrainer
    participant Evaluator as Evaluator
    participant Visualizer as Visualizer
    participant Predictor as Predictor

    Note over User,Predictor: è®­ç»ƒæµç¨‹
    User->>Train: æ‰§è¡Œè®­ç»ƒå‘½ä»¤
    Train->>Loader: load_data(è·¯å¾„, ç›®æ ‡åˆ—)
    Loader-->>Train: ç‰¹å¾æ•°æ®, ç›®æ ‡å€¼
    Train->>Prep: fit_transform(æ•°æ®, å‰”é™¤åˆ—)
    Prep-->>Train: å¤„ç†åæ•°æ®, ç‰¹å¾å
    Train->>Trainer: train(è®­ç»ƒæ•°æ®, æµ‹è¯•æ•°æ®, è¶…å‚æ•°)
    Trainer->>Trainer: cross-validation
    Trainer->>Trainer: Optunaä¼˜åŒ–
    Trainer-->>Train: è®­ç»ƒå¥½çš„æ¨¡å‹
    Train->>Evaluator: evaluate(y_true, y_pred)
    Evaluator-->>Train: RÂ², RMSE, COVç­‰
    Train->>Visualizer: plot_predictions(çœŸå®å€¼, é¢„æµ‹å€¼)
    Visualizer-->>Train: æ•£ç‚¹å›¾
    Train->>Visualizer: plot_feature_importance(ç‰¹å¾é‡è¦æ€§)
    Visualizer-->>Train: ç‰¹å¾é‡è¦æ€§å›¾
    Train->>Train: save_model(æ¨¡å‹, é¢„å¤„ç†å™¨, é…ç½®)
    Train-->>User: è®­ç»ƒå®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆ

    Note over User,Predictor: é¢„æµ‹æµç¨‹
    User->>Predictor: predict_single(è¾“å…¥å­—å…¸)
    Predictor->>Predictor: åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å™¨
    Predictor->>Evaluator: calculate_cov(y_true, y_pred)
    Evaluator-->>Predictor: COVå€¼
    Predictor-->>User: é¢„æµ‹ç»“æœ + ç½®ä¿¡åº¦

    Note over User,Predictor: ç‰¹å¾é€‰æ‹©æµç¨‹
    User->>Train: æ‰§è¡Œç‰¹å¾é€‰æ‹©å‘½ä»¤
    loop è¿­ä»£è®­ç»ƒ
        Train->>Trainer: train_and_evaluate(å½“å‰ç‰¹å¾é›†)
        Trainer-->>Train: æ¨¡å‹ + æ€§èƒ½æŒ‡æ ‡
        Train->>Train: ç§»é™¤é‡è¦æ€§æœ€ä½ç‰¹å¾
    end
    Train->>Visualizer: plot_feature_selection_curve(ç»“æœ)
    Visualizer-->>Train: ç‰¹å¾é€‰æ‹©æ›²çº¿
    Train-->>User: æœ€ä¼˜ç‰¹å¾é›†æ¨è
```

---

## é…ç½®æ–‡ä»¶è¯¦è§£

### ä¸»é…ç½®æ–‡ä»¶ï¼ˆconfig.yamlï¼‰

```yaml
# æ•°æ®é…ç½®
data:
  train_path: "feature_parameters.csv"         # è®­ç»ƒæ•°æ®è·¯å¾„
  target_column: "Nexp (kN)"                   # ç›®æ ‡åˆ—ï¼ˆæ‰¿è½½åŠ›ï¼‰
  test_size: 0.2                               # æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆ20%ï¼‰
  random_state: 42                             # éšæœºç§å­

  # å‰”é™¤çš„åˆ—ï¼ˆå‡ ä½•å‚æ•°ï¼‰
  columns_to_drop:                             #
    - "b (mm)"                                # æˆªé¢å®½åº¦
    - "h (mm)"                                # æˆªé¢é«˜åº¦
    - "r0 (mm)"                               # åœ†è§’åŠå¾„
    - "t (mm)"                                # é’¢ç®¡åšåº¦
    - "L (mm)"                                # æŸ±é•¿
    - "lambda"                                # é•¿ç»†æ¯”

# æ¨¡å‹é…ç½®
model:
  type: "xgboost"                              # æ¨¡å‹ç±»å‹

  # XGBoostè¶…å‚æ•°
  xgboost_params:
    objective: "reg:squarederror"              # ç›®æ ‡å‡½æ•°
    n_estimators: 200                          # æ ‘çš„æ•°é‡
    learning_rate: 0.1                         # å­¦ä¹ ç‡
    max_depth: 6                               # æ ‘æœ€å¤§æ·±åº¦
    min_child_weight: 3                        # æœ€å°å­èŠ‚ç‚¹æƒé‡
    subsample: 0.8                             # æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
    colsample_bytree: 0.8                      # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    gamma: 0                                   # æœ€å°æŸå¤±å‡å°‘
    reg_alpha: 0                               # L1æ­£åˆ™åŒ–
    reg_lambda: 1                              # L2æ­£åˆ™åŒ–
    max_delta_step: 0                          # æœ€å¤§å¢é‡æ­¥é•¿
    random_state: 42                           # éšæœºç§å­
    n_jobs: -1                                 # å¹¶è¡Œæ•°ï¼ˆ-1ä¸ºå…¨éƒ¨æ ¸å¿ƒï¼‰

# äº¤å‰éªŒè¯é…ç½®
cross_validation:
  k_folds: 5                                   # KæŠ˜äº¤å‰éªŒè¯
  scoring: "neg_root_mean_squared_error"       # è¯„åˆ†æŒ‡æ ‡
  random_state: 42

# è¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptunaï¼‰
optuna:
  enabled: true                                # æ˜¯å¦å¯ç”¨
  n_trials: 100                                # è¯•éªŒæ¬¡æ•°
  timeout: 3600                                # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
  study_name: "cfst_hyperopt"                  # ç ”ç©¶åç§°
  direction: "minimize"                        # ä¼˜åŒ–æ–¹å‘

  # æœç´¢ç©ºé—´
  search_space:
    n_estimators: {"type": "int", "low": 100, "high": 500, "step": 50}
    learning_rate: {"type": "float", "low": 0.01, "high": 0.3, "log": true}
    max_depth: {"type": "int", "low": 3, "high": 10, "step": 1}
    min_child_weight: {"type": "int", "low": 1, "high": 10, "step": 1}
    subsample: {"type": "float", "low": 0.6, "high": 1.0, "step": 0.1}
    colsample_bytree: {"type": "float", "low": 0.6, "high": 1.0, "step": 0.1}
    gamma: {"type": "float", "low": 0, "high": 0.3, "step": 0.05}
    reg_alpha: {"type": "float", "low": 0, "high": 1, "step": 0.1}

# è¾“å‡ºè·¯å¾„
paths:
  output_dir: "output"                         # è¾“å‡ºä¸»ç›®å½•
  model_dir: "output/xgboost_model"            # æ¨¡å‹ä¿å­˜è·¯å¾„
  plots_dir: "output/xgboost_model/plots"      # å›¾è¡¨è·¯å¾„
  prediction_file: "output/predictions.csv"    # é¢„æµ‹ç»“æœæ–‡ä»¶

# æ—¥å¿—é…ç½®
logging:
  level: "INFO"                                # æ—¥å¿—çº§åˆ«
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "output/ml_pipeline.log"               # æ—¥å¿—æ–‡ä»¶
  max_file_size: 10485760                      # å•ä¸ªæ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°ï¼ˆ10MBï¼‰
  backup_count: 5                              # å¤‡ä»½æ•°é‡
  console_output: true                         # æ§åˆ¶å°è¾“å‡º
```

---

## è®­ç»ƒä½¿ç”¨æ‰‹å†Œ

### è®­ç»ƒé˜¶æ®µè¯´æ˜

æœ¬é¡¹ç›®ä½¿ç”¨å•ä¸€é…ç½®æ–‡ä»¶ `config/config.yaml`ï¼Œé€šè¿‡ `use_optuna` å‚æ•°åˆ‡æ¢è®­ç»ƒé˜¶æ®µï¼š

**Stage 1: åŸºç¡€è®­ç»ƒ**ï¼ˆé»˜è®¤ï¼‰
- `use_optuna: false` - ä½¿ç”¨é¢„è®¾å‚æ•°è¿›è¡Œè®­ç»ƒ
- ç”¨äºè¯„ä¼°åŸºå‡†æ€§èƒ½

**Stage 2: Optunaæ·±åº¦è°ƒä¼˜**
- `use_optuna: true` - å¯ç”¨è¶…å‚æ•°è‡ªåŠ¨æœç´¢
- ç›®æ ‡ï¼šä¼˜åŒ–COVæŒ‡æ ‡è‡³ < 0.05

### æ­¥éª¤1ï¼šç¯å¢ƒå‡†å¤‡

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository-url>
cd xgboost

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate     # Windows

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### æ­¥éª¤2ï¼šæ•°æ®å‡†å¤‡

ç¡®ä¿æ•°æ®æ–‡ä»¶ `feature_parameters.csv` æ ¼å¼æ­£ç¡®ï¼š

| b (mm) | h (mm) | t (mm) | ... | Nexp (kN) |
|--------|--------|--------|-----|-----------|
| 100    | 100    | 3.0    | ... | 2500      |
| 120    | 120    | 4.0    | ... | 3200      |

### æ­¥éª¤3ï¼šé…ç½®æ¨¡å‹

ç¼–è¾‘ `config/config.yaml` æ–‡ä»¶ï¼š
- ä¿®æ”¹æ•°æ®è·¯å¾„ï¼ˆå¦‚éœ€ï¼‰
- è°ƒæ•´è¶…å‚æ•°
- é…ç½®äº¤å‰éªŒè¯å‚æ•°

### æ­¥éª¤4ï¼šæ‰§è¡Œè®­ç»ƒ

#### åŸºç¡€è®­ç»ƒï¼ˆå‰”é™¤å‡ ä½•å‚æ•°ï¼‰

```bash
# åŸºæœ¬è®­ç»ƒå‘½ä»¤
python train.py --config config/config.yaml

# è¾“å‡ºç¤ºä¾‹ï¼š
================================================================
          XGBoost ML Pipeline - CFSTæ‰¿è½½åŠ›é¢„æµ‹
================================================================
å‘½ä»¤è¡Œå‚æ•°:
  config: config/config.yaml
  verbose: True

æ­¥éª¤ 1: åŠ è½½æ•°æ®
  è®­ç»ƒæ•°æ®: feature_parameters.csv
  ç›®æ ‡åˆ—: Nexp (kN)
  æ•°æ®é›†å¤§å°: (1000, 24)
  ç‰¹å¾æ•°é‡: 24
  å‰”é™¤åˆ—: ['b (mm)', 'h (mm)', 'r0 (mm)', 't (mm)', 'L (mm)', 'lambda']
  ä¿ç•™ç‰¹å¾: 19

æ­¥éª¤ 2: æ•°æ®é¢„å¤„ç†
  å¡«å……ç¼ºå¤±å€¼ï¼ˆä¸­ä½æ•°æ’è¡¥ï¼‰
  æ‹†åˆ†æ•°æ®é›†: è®­ç»ƒé›†80%, æµ‹è¯•é›†20%

æ­¥éª¤ 3: æ¨¡å‹è®­ç»ƒ
  å¯ç”¨Optunaè¶…å‚æ•°ä¼˜åŒ–
  XGBoostå‚æ•°:
    - n_estimators: 200
    - learning_rate: 0.1
    - max_depth: 6
  è®­ç»ƒæ—¶é—´: 15.23ç§’

æ­¥éª¤ 4: æ¨¡å‹è¯„ä¼°
  RÂ² Score: 0.9876
  RMSE: 254.32 kN
  MAE: 187.45 kN
  MAPE: 8.45%
  COV: 0.0892 (Î¼=0.985, excellent stability)

æ­¥éª¤ 5: å¯è§†åŒ–
  ç”Ÿæˆé¢„æµ‹æ•£ç‚¹å›¾: output/xgboost_model/plots/predictions_scatter.png
  ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾: output/xgboost_model/plots/feature_importance.png

æ­¥éª¤ 6: ä¿å­˜æ¨¡å‹
  æ¨¡å‹ä¿å­˜åˆ°: output/xgboost_model/xgboost_model.pkl
  é¢„å¤„ç†å™¨ä¿å­˜åˆ°: output/xgboost_model/preprocessor.pkl
  è¯„ä¼°æŠ¥å‘Šä¿å­˜åˆ°: output/xgboost_model/evaluation_report.json

è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: 45.32ç§’
================================================================
```

#### å¿«é€Ÿè®­ç»ƒï¼ˆç¦ç”¨Optunaï¼‰

```bash
python train.py --config config/config.yaml
```

**åˆ‡æ¢åˆ° Stage 2ï¼ˆå¯ç”¨Optunaï¼‰**ï¼š
```bash
# ç¼–è¾‘ config.yamlï¼Œè®¾ç½® use_optuna: true
# ç„¶åæ‰§è¡Œï¼š
python train.py --config config/config.yaml
```

### æ­¥éª¤4ï¼šæŸ¥çœ‹è®­ç»ƒç»“æœ

è®­ç»ƒå®Œæˆåï¼ŒæŸ¥çœ‹è¾“å‡ºç›®å½•ï¼š

```bash
tree output/xgboost_model/
```

è¾“å‡ºç›®å½•ç»“æ„ï¼š
```
output/xgboost_model/
â”œâ”€â”€ xgboost_model.pkl           # è®­ç»ƒå¥½çš„æ¨¡å‹
â”œâ”€â”€ preprocessor.pkl            # é¢„å¤„ç†å™¨
â”œâ”€â”€ evaluation_report.json      # è¯„ä¼°æŠ¥å‘Š
â”œâ”€â”€ feature_names.json          # ç‰¹å¾åç§°åˆ—è¡¨
â””â”€â”€ plots/
    â”œâ”€â”€ predictions_scatter.png # é¢„æµ‹vsçœŸå®æ•£ç‚¹å›¾
    â”œâ”€â”€ predictions_residual.png # æ®‹å·®å›¾
    â””â”€â”€ feature_importance.png  # ç‰¹å¾é‡è¦æ€§å›¾
```

### æ­¥éª¤5ï¼šè§£è¯»è¯„ä¼°æŠ¥å‘Š

æ‰“å¼€ `output/xgboost_model/evaluation_report.json`ï¼š

```json
{
    "model_info": {
        "type": "xgboost",
        "trained_at": "2026-01-14T10:30:00",
        "training_time": 45.32
    },
    "test_metrics": {
        "r2_score": 0.9876,
        "rmse": 254.32,
        "mae": 187.45,
        "mape": 8.45,
        "cov": 0.0892,
        "cov_mean_ratio": 0.985
    },
    "cv_metrics": {
        "mean_rmse": 267.85,
        "std_rmse": 23.45
    }
}
```

**å…³é”®æŒ‡æ ‡è§£è¯»**ï¼š
- **RÂ² (0.9876)**ï¼šä¼˜ç§€ï¼Œè§£é‡Š98.76%çš„æ–¹å·®
- **RMSE (254.32)**ï¼šå¹³å‡é¢„æµ‹è¯¯å·®254.32kN
- **COV (0.0892)**ï¼šä¼˜ç§€ï¼Œé¢„æµ‹ç¨³å®šæ€§å¾ˆå¥½ï¼ˆ<0.10ï¼‰
- **COV Mean Ratio (0.985)**ï¼šæ¥è¿‘1.0ï¼Œæ— ç³»ç»Ÿæ€§åå·®

---

## Optuna æŒä¹…åŒ–å­˜å‚¨ä¸æ™ºèƒ½å‚æ•°åŠ è½½

### åŠŸèƒ½æ¦‚è¿°

æœ¬é¡¹ç›®é›†æˆäº† Optuna æŒä¹…åŒ–å­˜å‚¨å’Œæ™ºèƒ½å‚æ•°åŠ è½½åŠŸèƒ½ï¼Œå®ç°äº†ï¼š
- **æ–­ç‚¹ç»­è®­**ï¼šOptuna ä¼˜åŒ–è¿‡ç¨‹è‡ªåŠ¨ä¿å­˜åˆ° SQLite æ•°æ®åº“ï¼Œå¯éšæ—¶ä¸­æ–­å’Œæ¢å¤
- **å‚æ•°è®°å½•**ï¼šæœ€ä¼˜å‚æ•°è‡ªåŠ¨ä¿å­˜åˆ° JSON æ–‡ä»¶ï¼Œä¾¿äºæŸ¥çœ‹å’Œå¤ç”¨
- **æ™ºèƒ½åŠ è½½**ï¼šå½“ `use_optuna: false` æ—¶ï¼Œè‡ªåŠ¨åŠ è½½æœ€ä¼˜å‚æ•°è¿›è¡Œè®­ç»ƒ

### æ–‡ä»¶ç»“æ„

```
logs/
â”œâ”€â”€ optuna_study.db      # Optuna å®Œæ•´ study æ•°æ®åº“ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
â””â”€â”€ best_params.json     # æœ€ä¼˜å‚æ•°è®°å½•ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
```

### ä½¿ç”¨æ–¹å¼

#### æ–¹å¼1ï¼šæ‰§è¡Œ Optuna è°ƒä¼˜ï¼ˆè‡ªåŠ¨ä¿å­˜ï¼‰

```bash
# 1. è®¾ç½® use_optuna: true
# ç¼–è¾‘ config/config.yaml:
# model:
#   use_optuna: true

# 2. è¿è¡Œè®­ç»ƒ
python train.py --config config/config.yaml

# 3. è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶ï¼š
#    - logs/optuna_study.dbï¼ˆå®Œæ•´çš„ trials è®°å½•ï¼‰
#    - logs/best_params.jsonï¼ˆæœ€ä¼˜å‚æ•°å¿«ç…§ï¼‰
```

**æ—¥å¿—è¾“å‡ºç¤ºä¾‹**ï¼š
```
INFO:src.utils.model_utils:Best parameters saved to logs/best_params.json
INFO:src.utils.model_utils:  Best RMSE: 554.9208 (Trial 79)
INFO:src.utils.model_utils:  Timestamp: 2026-01-14T12:32:00
```

#### æ–¹å¼2ï¼šä½¿ç”¨å·²ä¿å­˜çš„æœ€ä¼˜å‚æ•°ï¼ˆè‡ªåŠ¨åŠ è½½ï¼‰

```bash
# 1. è®¾ç½® use_optuna: false
# ç¼–è¾‘ config/config.yaml:
# model:
#   use_optuna: false

# 2. è¿è¡Œè®­ç»ƒï¼ˆè‡ªåŠ¨åŠ è½½æœ€ä¼˜å‚æ•°ï¼‰
python train.py --config config/config.yaml
```

**æ—¥å¿—è¾“å‡ºç¤ºä¾‹**ï¼š
```
INFO:src.utils.model_utils:Loaded best parameters from logs/best_params.json
INFO:src.utils.model_utils:  Best RMSE: 554.9208 (Trial 79)
INFO:src.utils.model_utils:  Saved on: 2026-01-14T12:32:00
INFO:src.model_trainer:Using loaded best parameters for training
```

#### æ–¹å¼3ï¼šæ–­ç‚¹ç»­è®­ï¼ˆç´¯ç§¯ trialsï¼‰

```bash
# ç¬¬ä¸€æ¬¡è¿è¡Œï¼š100 trials
python train.py --config config/config.yaml
# è¾“å‡ºï¼šå·²å®Œæˆ 100 trialsï¼Œæœ€ä¼˜ RMSE: 554.92

# ç¬¬äºŒæ¬¡è¿è¡Œï¼šç»§ç»­å¢åŠ  50 trials
python train.py --config config/config.yaml
# è¾“å‡ºï¼šåŠ è½½å·²æœ‰ studyï¼Œå·²å®Œæˆ 150 trials
```

### best_params.json æ–‡ä»¶æ ¼å¼

```json
{
  "trial_number": 79,
  "best_rmse": 554.9208295740007,
  "parameters": {
    "max_depth": 7,
    "learning_rate": 0.10456477089212307,
    "n_estimators": 547,
    "subsample": 0.9059708763991396,
    "colsample_bytree": 0.8685274806023014,
    "min_child_weight": 20,
    "reg_alpha": 1.1408620775128826,
    "reg_lambda": 3.5871489124615974,
    "gamma": 0.09873345991294272
  },
  "timestamp": "2026-01-14T12:32:00",
  "n_trials_total": 100
}
```

### ä¼˜åŠ¿å¯¹æ¯”

| åŠŸèƒ½ | ä¼ ç»Ÿæ–¹å¼ | æœ¬é¡¹ç›®å®ç° |
|------|---------|-----------|
| å‚æ•°æŒä¹…åŒ– | âŒ æ¯æ¬¡é‡æ–°å¼€å§‹ | âœ… SQLite æ•°æ®åº“æŒä¹…åŒ– |
| å‚æ•°å¤ç”¨ | âŒ æ‰‹åŠ¨å¤åˆ¶ç²˜è´´ | âœ… è‡ªåŠ¨åŠ è½½ JSON æ–‡ä»¶ |
| æ–­ç‚¹ç»­è®­ | âŒ ä¸­æ–­åé‡æ–°è¿è¡Œ | âœ… è‡ªåŠ¨æ¢å¤å¹¶ç´¯ç§¯ trials |
| å‚æ•°è¿½æº¯ | âŒ åªæœ‰æ—¥å¿—è®°å½• | âœ… å®Œæ•´çš„ trials å†å² |

### æ³¨æ„äº‹é¡¹

1. **logs/ ç›®å½•è‡ªåŠ¨åˆ›å»º**ï¼šé¦–æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨åˆ›å»º
2. **æ–‡ä»¶è¦†ç›–**ï¼šæ¯æ¬¡è¿è¡Œä¼šæ›´æ–° `best_params.json` å’Œ `optuna_study.db`
3. **å‚æ•°ä¼˜å…ˆçº§**ï¼š`use_optuna: false` æ—¶ï¼ŒåŠ è½½çš„å‚æ•°ä¼šè¦†ç›– config.yaml ä¸­çš„é»˜è®¤å‚æ•°

---

## é¢„æµ‹ä½¿ç”¨æ‰‹å†Œ

### æ–¹å¼1ï¼šæ‰¹é‡é¢„æµ‹ï¼ˆCSVæ–‡ä»¶ï¼‰

```bash
python predict.py --model output/xgboost_model --input all.csv --output predictions.csv
```

å‚æ•°è¯´æ˜ï¼š
- `--model`ï¼šæ¨¡å‹ç›®å½•è·¯å¾„
- `--input`ï¼šè¾“å…¥CSVæ–‡ä»¶è·¯å¾„
- `--output`ï¼šè¾“å‡ºé¢„æµ‹ç»“æœæ–‡ä»¶è·¯å¾„
- `--batch-size`ï¼šæ‰¹é‡å¤§å°ï¼ˆé»˜è®¤1000ï¼‰

è¾“å‡ºCSVæ ¼å¼ï¼š
```csv
index,prediction_Nexp (kN)
0,2500.5
1,3200.2
2,2850.7
...
```

### æ–¹å¼2ï¼šå•æ¡é¢„æµ‹ï¼ˆäº¤äº’å¼ï¼‰

```bash
python predict.py --model output/xgboost_model --single
```

äº¤äº’è¾“å…¥ç¤ºä¾‹ï¼š
```
è¯·è¾“å…¥é¢„æµ‹å‚æ•°ï¼ˆæ ¼å¼ï¼škey=valueï¼Œæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š
fc (MPa)=40.5
fy (MPa)=350.2
Ac (mm^2)=10000
As (mm^2)=500
...

é¢„æµ‹ç»“æœï¼š
æ‰¿è½½åŠ›: 2850.7 kN
ç½®ä¿¡åŒºé—´: [2720.5, 2980.9] kN
COV: 0.089 (ä¼˜ç§€)
```

### æ–¹å¼3ï¼šPython APIè°ƒç”¨

```python
from src.predictor import Predictor

# åˆå§‹åŒ–é¢„æµ‹å™¨
predictor = Predictor("output/xgboost_model")

# å•æ¡é¢„æµ‹
input_data = {
    "fc (MPa)": 40.5,
    "fy (MPa)": 350.2,
    "Ac (mm^2)": 10000,
    # ... å…¶ä»–å‚æ•°
}
result = predictor.predict_single(input_data)
print(f"é¢„æµ‹æ‰¿è½½åŠ›: {result['prediction']:.2f} kN")
print(f"COV: {result['cov']:.4f}")

# æ‰¹é‡é¢„æµ‹
import pandas as pd
df = pd.read_csv("input_data.csv")
predictions = predictor.predict_batch(df, save_to="batch_predictions.csv")
```

### é¢„æµ‹ç»“æœè§£è¯»

é¢„æµ‹è¾“å‡ºåŒ…å«ï¼š
- **prediction**: é¢„æµ‹æ‰¿è½½åŠ›ï¼ˆkNï¼‰
- **confidence_interval**: 95%ç½®ä¿¡åŒºé—´ï¼ˆåŸºäºCOVè®¡ç®—ï¼‰
- **cov**: å˜å¼‚ç³»æ•°ï¼Œè¯„ä¼°é¢„æµ‹ç¨³å®šæ€§

**COVç­‰çº§**ï¼š
- **< 0.05**: æå¥½
- **0.05-0.10**: ä¼˜ç§€ï¼ˆæ¨èéƒ¨ç½²é˜ˆå€¼ï¼‰
- **0.10-0.15**: è‰¯å¥½
- **0.15-0.20**: å¯æ¥å—
- **> 0.20**: ä¸ç¨³å®š

---

## ç‰¹å¾é€‰æ‹©ç®¡é“ä½¿ç”¨æ‰‹å†Œ

### ä¸ºä»€ä¹ˆéœ€è¦ç‰¹å¾é€‰æ‹©ï¼Ÿ

ç‰¹å¾é€‰æ‹©ç®¡é“é€šè¿‡è¿­ä»£å‰”é™¤æœ€ä¸é‡è¦çš„ç‰¹å¾ï¼Œå¸®åŠ©æ‰¾åˆ°ï¼š
- **æœ€ä¼˜ç‰¹å¾å­é›†**ï¼šåœ¨å‡†ç¡®æ€§å’Œå¤æ‚åº¦é—´å¹³è¡¡
- **å…³é”®å½±å“å› ç´ **ï¼šè¯†åˆ«æœ€é‡è¦çš„å·¥ç¨‹å‚æ•°
- **æ¨¡å‹ç®€åŒ–**ï¼šå‡å°‘è¿‡æ‹Ÿåˆé£é™©ï¼Œæé«˜é¢„æµ‹æ•ˆç‡

### å¿«é€Ÿå¼€å§‹

```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶è¿è¡Œç‰¹å¾é€‰æ‹©
python feature_selection_pipeline.py --config config/config.yaml
```

### é«˜çº§é…ç½®

```bash
# æŒ‡å®šè¾“å‡ºç›®å½•
python feature_selection_pipeline.py --config config/config.yaml --output-dir output/my_feature_selection

# æŒ‡å®šæœ€å¤§ç‰¹å¾æ•°ï¼ˆè‡ªåŠ¨åœæ­¢ï¼‰
python feature_selection_pipeline.py --config config/config.yaml --max-features 15

# è°ƒæ•´äº¤å‰éªŒè¯æŠ˜æ•°
python feature_selection_pipeline.py --config config/config.yaml --cv-folds 10
```

### ç®¡é“æ‰§è¡Œæµç¨‹

```
å¼€å§‹
  â†“
åŠ è½½é…ç½®å’Œæ•°æ®
  â†“
åˆå§‹åŒ–ï¼šä½¿ç”¨å…¨éƒ¨ç‰¹å¾è®­ç»ƒæ¨¡å‹
  â†“
è®°å½•æ€§èƒ½æŒ‡æ ‡ï¼ˆRÂ², COVç­‰ï¼‰
  â†“
è·å–ç‰¹å¾é‡è¦æ€§æ’åº
  â†“
å‰”é™¤é‡è¦æ€§æœ€ä½ç‰¹å¾ï¼ˆ1ä¸ªï¼‰
  â†“
ç”¨å‰©ä½™ç‰¹å¾é‡æ–°è®­ç»ƒ
  â†“
é‡å¤è¿­ä»£ç›´åˆ°åªå‰©1ä¸ªç‰¹å¾
  â†“
ç”Ÿæˆæ€§èƒ½æ›²çº¿å›¾
  â†“
è¯†åˆ«æœ€ä¼˜å­é›†ï¼ˆBest RÂ², Best CV, Elbowï¼‰
  â†“
è¾“å‡ºæ€»ç»“æŠ¥å‘Š
  â†“
ç»“æŸ
```

### è¾“å‡ºç»“æœè§£è¯»

ç‰¹å¾é€‰æ‹©å®Œæˆåï¼ŒæŸ¥çœ‹è¾“å‡ºç›®å½•ï¼š

```bash
tree output/feature_selection/
```

è¾“å‡ºå†…å®¹ï¼š

```
output/feature_selection/
â”œâ”€â”€ feature_selection_results.csv   # æ¯æ¬¡è¿­ä»£çš„æ€§èƒ½è®°å½•
â”œâ”€â”€ detailed_results.json           # è¯¦ç»†ç»“æœï¼ˆJSONï¼‰
â”œâ”€â”€ feature_selection_curves.png    # æ€§èƒ½æ›²çº¿å›¾ï¼ˆå¸¦COVï¼‰
â”œâ”€â”€ performance_summary.png         # æ€§èƒ½å¯¹æ¯”å›¾
â”œâ”€â”€ summary_report.txt              # æ€»ç»“æŠ¥å‘Š
â”œâ”€â”€ optimal_subsets.json            # æœ€ä¼˜å­é›†é…ç½®
â””â”€â”€ iteration_001-iteration_018/    # æ¯ä¸ªè¿­ä»£çš„è¯¦ç»†ç»“æœ
    â”œâ”€â”€ model.pkl
    â”œâ”€â”€ evaluation_report.json
    â””â”€â”€ plots/
```

### æ€§èƒ½æ›²çº¿åˆ†æ

ç‰¹å¾é€‰æ‹©æ›²çº¿å›¾ï¼ˆ5ä¸ªå­å›¾ï¼‰ï¼š

1. **RÂ² vs ç‰¹å¾æ•°é‡**ï¼šè¯†åˆ«æœ€ä½³RÂ²å­é›†
2. **RMSE vs ç‰¹å¾æ•°é‡**ï¼šè¯„ä¼°é¢„æµ‹è¯¯å·®
3. **æœ€ä¼˜CV vs ç‰¹å¾æ•°é‡**ï¼šäº¤å‰éªŒè¯ä¼˜åŒ–
4. **ç‰¹å¾é‡è¦æ€§å æ¯”**ï¼šç´¯ç§¯é‡è¦æ€§
5. **COV vs ç‰¹å¾æ•°é‡**ï¼šğŸ”¥ æ–°å¢-è¯„ä¼°é¢„æµ‹ç¨³å®šæ€§

### æœ€ä¼˜å­é›†æ¨è

æŸ¥çœ‹ `summary_report.txt`ï¼š

```
FEATURE SELECTION SUMMARY REPORT
================================

è¿­ä»£æ¬¡æ•°: 18
åˆå§‹ç‰¹å¾æ•°: 18
æœ€ç»ˆç‰¹å¾æ•°: 1

OPTIMAL FEATURE SUBSETS:
-----------------------

1. Best RÂ² Score:                    # RÂ²æœ€ä¼˜
   - Iteration: 2
   - Features: 17                    # 17ä¸ªç‰¹å¾
   - RÂ² Score: 0.9964                # RÂ²=0.9964
   - RMSE: 204.24                    # RMSE=204.24
   - COV: 0.1049                     # COV=0.1049ï¼ˆä¼˜ç§€ï¼‰
   - Feature list: fy, fc, ...

2. Best Cross-Validation:            # äº¤å‰éªŒè¯æœ€ä¼˜
   - Iteration: 2
   - Features: 17
   - CV RMSE: -630.42                # è´ŸRMSE
   - RÂ²: 0.9964
   - COV: 0.1049
   - Feature list: fy, fc, ...

3. Elbow Method:                     # è‚˜éƒ¨æ³•
   - Iteration: 11
   - Features: 8                     # ä»…8ä¸ªå…³é”®ç‰¹å¾
   - RÂ² Score: 0.9916                # ä¿æŒ0.9916
   - COV: 0.1629                     # ç¨³å®šæ€§å¯æ¥å—
   - Improvement Rate: 0.0002        # æå‡å·²å¹³ç¼“
   - Feature list: Ac, As, Re, ...

RECOMMENDATIONS:
---------------
- If maximizing accuracy: Use Best RÂ² subset (17 features)
- If balancing accuracy and simplicity: Use Elbow (8 features)
- For production: Consider Best CV for generalization

COV INTERPRETATION:
------------------
- COV < 0.10: æå¥½
- COV < 0.15: è‰¯å¥½
- COV < 0.20: å¯æ¥å—
```

### å¦‚ä½•é€‰æ‹©æœ€ä¼˜å­é›†ï¼Ÿ

**å·¥ç¨‹å®è·µå»ºè®®**ï¼š

1. **æœ€ä½³å‡†ç¡®åº¦**ï¼šé€‰æ‹©**Best RÂ²**ï¼ˆ17ä¸ªç‰¹å¾ï¼‰
   - RÂ² = 0.9964ï¼ˆå‡ ä¹æ— æ€§èƒ½æŸå¤±ï¼‰
   - COV = 0.1049ï¼ˆä¼˜ç§€ç¨³å®šæ€§ï¼‰

2. **å¹³è¡¡é€‰æ‹©**ï¼šé€‰æ‹©**Elbow Method**ï¼ˆ8ä¸ªç‰¹å¾ï¼‰
   - RÂ² = 0.9916ï¼ˆä»…æŸå¤±0.5%ï¼‰
   - COV = 0.1629ï¼ˆè‰¯å¥½ç¨³å®šæ€§ï¼‰
   - **å¤æ‚åº¦é™ä½55.6%**

3. **ç”Ÿäº§éƒ¨ç½²**ï¼šé€‰æ‹©**Best CV**ï¼ˆ17ä¸ªç‰¹å¾ï¼‰
   - äº¤å‰éªŒè¯æœ€ä¼˜ï¼Œæ³›åŒ–èƒ½åŠ›æœ€å¥½
   - COV = 0.1049

### å¯¹æ¯”åˆ†æ

ä½¿ç”¨å¯¹æ¯”é…ç½®æ–‡ä»¶æŸ¥çœ‹ä¸¤ç§ç­–ç•¥çš„å·®å¼‚ï¼š

```bash
# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
cat output/feature_selection_comparison.csv
```

å¯¹æ¯”ç»“æœè§£è¯»ï¼š

| æŒ‡æ ‡           | æ— å‡ ä½•å‚æ•° | æœ‰å‡ ä½•å‚æ•° | è¯´æ˜                 |
| -------------- | ---------- | ---------- | -------------------- |
| åˆå§‹ç‰¹å¾æ•°     | 18         | 24         | å‰”é™¤6ä¸ªå‡ ä½•å‚æ•°      |
| Best RÂ²        | 0.9964     | 0.9965     | **å‡ ä¹æ— æ€§èƒ½æŸå¤±**   |
| Bestç‰¹å¾æ•°     | 17         | 21         | æœ€ä¼˜å­é›†å¤§å°         |
| Best RMSE      | 204.24     | 200.53     | è¯¯å·®ç›¸è¿‘             |
| **Best COV**   | **0.1049** | **0.1116** | **æ— å‡ ä½•å‚æ•°æ›´å¥½**   |

**æ ¸å¿ƒå‘ç°**ï¼šå‰”é™¤å‡ ä½•å‚æ•°åï¼Œä»…ç”¨18ä¸ªæ— é‡çº²å‚æ•°å³å¯è¾¾åˆ°ç›¸åŒç²¾åº¦ï¼Œè¯æ˜å¤šæˆªé¢ç»Ÿä¸€é¢„æµ‹æ¨¡å‹å¯è¡Œï¼

---

## è¯¦ç»†å‚æ•°è¯´æ˜

### ä¿ç•™çš„18ä¸ªæ— é‡çº²å‚æ•°

| å‚æ•°å          | å•ä½ | ç‰©ç†æ„ä¹‰                     | å·¥ç¨‹æ„ä¹‰             |
| --------------- | ---- | ---------------------------- | -------------------- |
| R               | %    | å†ç”Ÿç²—éª¨æ–™å–ä»£ç‡             | ç¯ä¿ææ–™æ¯”ä¾‹         |
| fy              | MPa  | é’¢æå±ˆæœå¼ºåº¦                 | é’¢æå¼ºåº¦             |
| fc              | MPa  | æ··å‡åœŸæŠ—å‹å¼ºåº¦               | æ··å‡åœŸå¼ºåº¦           |
| e1              | mm   | ä¸Šç«¯åå¿ƒè·                   | ä¸Šéƒ¨åå¿ƒ             |
| e2              | mm   | ä¸‹ç«¯åå¿ƒè·                   | ä¸‹éƒ¨åå¿ƒ             |
| r0/h            | -    | è§’å¾„æ¯”                       | æˆªé¢å½¢çŠ¶             |
| b/t             | -    | å¾„åšæ¯”                       | ç®¡å£ç›¸å¯¹åšåº¦         |
| Ac              | mmÂ²  | ç­‰æ•ˆæ ¸å¿ƒæ··å‡åœŸé¢ç§¯           | æ··å‡åœŸé¢ç§¯           |
| As              | mmÂ²  | ç­‰æ•ˆé’¢ç®¡é¢ç§¯                 | é’¢æé¢ç§¯             |
| Re              | mm   | ç­‰æ•ˆæ··å‡åœŸåŠå¾„               | ç­‰æ•ˆåŠå¾„             |
| te              | mm   | ç­‰æ•ˆé’¢ç®¡åšåº¦                 | ç­‰æ•ˆåšåº¦             |
| ke              | -    | çº¦æŸæœ‰æ•ˆæ€§ç³»æ•°               | çº¦æŸæ•ˆæœ             |
| xi              | -    | å¥—ç®ç³»æ•°                     | å¥—ç®ä½œç”¨             |
| sigma_re        | MPa  | æœ‰æ•ˆä¾§å‘åº”åŠ›                 | çº¦æŸåº”åŠ›             |
| lambda_bar      | -    | ç›¸å¯¹é•¿ç»†æ¯”                   | é•¿ç»†æ¯”               |
| e/h             | -    | æœ€å¤§åå¿ƒç‡                   | åå¿ƒç¨‹åº¦             |
| e1/e2           | -    | åå¿ƒæ¯”ç‡                     | ä¸Šä¸‹åå¿ƒæ¯”           |
| e_bar           | -    | ç›¸å¯¹åå¿ƒç‡                   | ç›¸å¯¹åå¿ƒ             |

### è¢«å‰”é™¤çš„6ä¸ªå‡ ä½•å‚æ•°

è¿™äº›å‚æ•°å› ä¸æ‰¿è½½èƒ½åŠ›éçº¿æ€§å…³ç³»ï¼Œè¢«å‰”é™¤ä»¥å®ç°å¤šæˆªé¢ç»Ÿä¸€é¢„æµ‹ï¼š

- **b (mm)**: æˆªé¢å®½åº¦
- **h (mm)**: æˆªé¢é«˜åº¦
- **r0 (mm)**: åœ†è§’åŠå¾„
- **t (mm)**: é’¢ç®¡åšåº¦
- **L (mm)**: æŸ±é•¿
- **lambda**: é•¿ç»†æ¯”ï¼ˆåŸå§‹å€¼ï¼‰

---

## COVï¼ˆå˜å¼‚ç³»æ•°ï¼‰è¯¦è§£

### COVè®¡ç®—åŸç†

```python
def calculate_cov(y_true, y_pred):
    """
    è®¡ç®—å˜å¼‚ç³»æ•°ï¼ˆCoefficient of Variationï¼‰

    æ­¥éª¤ï¼š
    1. è®¡ç®—é¢„æµ‹/çœŸå®æ¯”å€¼ï¼šÎ¾_i = y_pred_i / y_true_i
    2. è®¡ç®—å‡å€¼ï¼šÎ¼ = mean(Î¾_i)
    3. è®¡ç®—æ ‡å‡†å·®ï¼šÏƒ = std(Î¾_i)
    4. COV = Ïƒ / Î¼
    """
    ratios = y_pred / y_true
    mean_ratio = np.mean(ratios)
    std_ratio = np.std(ratios)
    cov = std_ratio / mean_ratio
    return cov
```

### COVç‰©ç†æ„ä¹‰

- **Î¼ â‰ˆ 1.0**: æ— ç³»ç»Ÿæ€§åå·®ï¼ˆç†æƒ³çŠ¶æ€ï¼‰
- **COV < 0.10**: æå¥½çš„é¢„æµ‹ç¨³å®šæ€§ï¼ˆæ¨èéƒ¨ç½²é˜ˆå€¼ï¼‰
- **Î¾ > 1.0**: é¢„æµ‹å€¼å¤§äºçœŸå®å€¼ï¼ˆå¯èƒ½ä¸å®‰å…¨ï¼‰
- **Î¾ < 1.0**: é¢„æµ‹å€¼å°äºçœŸå®å€¼ï¼ˆåä¿å®ˆ/å®‰å…¨ï¼‰

### å·¥ç¨‹æ ‡å‡†

åœ¨ç»“æ„å·¥ç¨‹ä¸­ï¼š
- **COV < 0.10**: ä¼˜ç§€æ¨¡å‹ï¼Œå¯ç”¨äºç”Ÿäº§
- **COV < 0.15**: è‰¯å¥½æ¨¡å‹ï¼Œéœ€å¢åŠ å®‰å…¨è£•åº¦
- **COV > 0.20**: ä¸ç¨³å®šï¼Œä¸å»ºè®®ä½¿ç”¨

---

## å¸¸è§é—®é¢˜è§£ç­”

### Q1: æ¨¡å‹è®­ç»ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ

**A**: å–å†³äºæ•°æ®é‡å’Œé…ç½®ï¼š
- æ— Optunaï¼ˆé»˜è®¤å‚æ•°ï¼‰ï¼š~30ç§’
- å¯ç”¨Optunaï¼ˆ100æ¬¡è¯•éªŒï¼‰ï¼š~15-30åˆ†é’Ÿ
- ç‰¹å¾é€‰æ‹©ï¼ˆ18æ¬¡è¿­ä»£ï¼‰ï¼š~5-10åˆ†é’Ÿ

### Q2: å¦‚ä½•å¤„ç†ç¼ºå¤±å€¼ï¼Ÿ

**A**: é¢„å¤„ç†å™¨è‡ªåŠ¨ä½¿ç”¨ä¸­ä½æ•°å¡«å……ï¼š
```python
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')
```

### Q3: æ¨¡å‹æ–‡ä»¶å¯ä»¥åœ¨å…¶ä»–è®¡ç®—æœºä½¿ç”¨å—ï¼Ÿ

**A**: å¯ä»¥ï¼Œéœ€æ»¡è¶³ï¼š
1. ç›¸åŒçš„Pythonç‰ˆæœ¬
2. å®‰è£…ç›¸åŒçš„ä¾èµ–ï¼ˆrequirements.txtï¼‰
3. ç›¸åŒçš„æ•°æ®æ ¼å¼ï¼ˆç‰¹å¾åç§°ä¸€è‡´ï¼‰

### Q4: å¦‚ä½•è°ƒæ•´æ¨¡å‹å¤æ‚åº¦ï¼Ÿ

**A**: ä¿®æ”¹ `config/config.yaml`ï¼š
```yaml
model:
  xgboost_params:
    n_estimators: 100    # å‡å°‘æ ‘çš„æ•°é‡
    max_depth: 4         # é™åˆ¶æ ‘æ·±åº¦
    min_child_weight: 5  # å¢åŠ æœ€å°å¶å­æƒé‡
```

### Q5: ç‰¹å¾é€‰æ‹©åå¦‚ä½•é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Ÿ

**A**: ä½¿ç”¨æœ€ä¼˜å­é›†é‡æ–°è®­ç»ƒï¼š

```python
# ä»JSONè¯»å–æœ€ä¼˜ç‰¹å¾
import json
with open("output/feature_selection/optimal_subsets.json", "r") as f:
    subsets = json.load(f)

best_features = subsets["best_r2"]["features"]

# é‡æ–°è®­ç»ƒï¼ˆå‚è€ƒ train.py å®ç°ï¼‰
```

### Q6: å¦‚ä½•è§£é‡Šç‰¹å¾é‡è¦æ€§ï¼Ÿ

**A**: XGBoostç‰¹å¾é‡è¦æ€§åŸºäºï¼š
1. **Gain**: ç‰¹å¾åœ¨åˆ†è£‚ä¸­çš„å¹³å‡å¢ç›Š
2. **Frequency**: ä½œä¸ºåˆ†è£‚ç‰¹å¾çš„æ¬¡æ•°
3. **Cover**: è¦†ç›–çš„æ ·æœ¬æ¯”ä¾‹

åœ¨CFSTä¸­ï¼Œé€šå¸¸æœ€é‡è¦çš„ç‰¹å¾ï¼š
- `fc` (æ··å‡åœŸå¼ºåº¦)
- `fy` (é’¢æå¼ºåº¦)
- `lambda_bar` (é•¿ç»†æ¯”)
- `e/h` (åå¿ƒç‡)

---

## æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°ç‰¹å¾

1. **ä¿®æ”¹æ•°æ®æ–‡ä»¶**ï¼šåœ¨ `feature_parameters.csv` æ·»åŠ æ–°åˆ—
2. **æ›´æ–°é…ç½®æ–‡ä»¶**ï¼šå¦‚æœä¸éœ€è¦å‰”é™¤ï¼Œæ— éœ€ä¿®æ”¹
3. **é‡æ–°è®­ç»ƒ**ï¼šæ‰§è¡Œè®­ç»ƒå‘½ä»¤
4. **éªŒè¯**ï¼šæ£€æŸ¥æ¨¡å‹æ€§èƒ½æå‡

### æ›´æ¢æ¨¡å‹ç®—æ³•

ç›®å‰ä»…æ”¯æŒXGBoostï¼Œå¦‚éœ€æ›´æ¢ï¼š

```python
# ä¿®æ”¹ src/model_trainer.py
class ModelTrainer:
    def __init__(self, config):
        # æ›´æ¢ä¸ºå…¶ä»–æ¨¡å‹
        from sklearn.ensemble import RandomForestRegressor
        self.model = RandomForestRegressor(**config)
```

### è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

æ·»åŠ æ–°æŒ‡æ ‡åˆ° `src/evaluator.py`ï¼š

```python
def calculate_custom_metric(self, y_true, y_pred):
    """è‡ªå®šä¹‰æŒ‡æ ‡"""
    # å®ç°è‡ªå®šä¹‰é€»è¾‘
    return metric_value
```

### é›†æˆåˆ°WebæœåŠ¡

```python
from flask import Flask, request, jsonify
from src.predictor import Predictor

app = Flask(__name__)
predictor = Predictor("output/xgboost_model")

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    result = predictor.predict_single(data)
    return jsonify(result)

if __name__ == '__main__':
    app.run(port=5000)
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–

1. **å‡å°‘Optunaè¯•éªŒæ¬¡æ•°**ï¼š
   ```yaml
   optuna:
     n_trials: 50  # ä»100å‡å°‘åˆ°50
   ```

2. **å‡å°‘äº¤å‰éªŒè¯æŠ˜æ•°**ï¼š
   ```yaml
   cross_validation:
     k_folds: 3  # ä»5å‡å°‘åˆ°3
   ```

3. **å¯ç”¨GPUåŠ é€Ÿ**ï¼š
   ```yaml
   model:
     xgboost_params:
       tree_method: "gpu_hist"  # ä½¿ç”¨GPU
   ```

### é¢„æµ‹é€Ÿåº¦ä¼˜åŒ–

1. **æ‰¹é‡é¢„æµ‹**ï¼š
   ```python
   # é¿å…é€æ¡é¢„æµ‹
   predictions = predictor.predict_batch(df)  # å¿«

   # é¿å…
   for index, row in df.iterrows():
       predictor.predict_single(row.to_dict())  # æ…¢
   ```

2. **æ¨¡å‹é‡åŒ–**ï¼šä½¿ç”¨XGBoostçš„æ¨¡å‹å‹ç¼©åŠŸèƒ½

3. **ç‰¹å¾ç¼“å­˜**ï¼šå¯¹é‡å¤é¢„æµ‹çš„æ•°æ®ç¼“å­˜é¢„å¤„ç†ç»“æœ

---

## è®¸å¯è¯

MIT License

---

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»ï¼š
- é¡¹ç›®ç»´æŠ¤è€…ï¼šYour Name
- é‚®ç®±ï¼šyour.email@example.com

---

## ç‰ˆæœ¬å†å²

### v1.0.0 (2026-01-14)
- âœ… å®Œæ•´XGBoost MLç®¡é“
- âœ… ç‰¹å¾é€‰æ‹©ç®¡é“
- âœ… COVå˜å¼‚ç³»æ•°é›†æˆ
- âœ… å¯¹æ¯”åˆ†ææŠ¥å‘Š
- âœ… å…¨é¢æ–‡æ¡£

### v0.9.0 (2026-01-13)
- âœ… åŸºç¡€è®­ç»ƒå’Œé¢„æµ‹åŠŸèƒ½
- âœ… æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
- âœ… æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–

---

**æ–‡æ¡£æœ€åæ›´æ–°**: 2026-01-14
**é¡¹ç›®çŠ¶æ€**: âœ… å·²å®Œæˆæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
